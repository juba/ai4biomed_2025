{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b38813-636c-4798-b086-8a786476b23b",
   "metadata": {},
   "source": [
    "# Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "from adl import model_2p\n",
    "from adl.sklearn import skl_regression\n",
    "\n",
    "pl.Config(tbl_rows=30, float_precision=2)\n",
    "pn.theme_set(pn.theme_minimal() + pn.theme(plot_background=pn.element_rect(fill=\"white\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f48e684-fd00-4a27-b0dc-188d918757a7",
   "metadata": {},
   "source": [
    "In this notebook we will take a look at a slightly more complicated\n",
    "model with two parameters, and at the effect of data normalization on\n",
    "the training process.\n",
    "\n",
    "## Data\n",
    "\n",
    "We first create a small nonsensical dataset with two numerical\n",
    "variables: the temperature and the quantity of ice cream sold at a shop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5368b6c2-5ecf-43f0-8c25-1cd73789637c",
   "metadata": {},
   "source": [
    "This time we will try to predict the `icecream` values from the\n",
    "`temperature` values with a simple linear model with both a slope and an\n",
    "intercept: $y = w \\times x + b$. Our model now has two parameters, a\n",
    "**weight** $w$, and a **bias** $b$.\n",
    "\n",
    "## Regression with scikit-learn\n",
    "\n",
    "As a reference, we first compute the “real” optimal slope and intercept\n",
    "values with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = skl_regression(x=temperature, y=icecream, fit_intercept=True)\n",
    "print(f\"slope: {reg['slope']:.2f}, intercept: {reg['intercept']:.2f}, mse: {reg['mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=temperature, y=icecream))\n",
    "    + pn.geom_hline(yintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_vline(xintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_abline(slope=reg[\"slope\"], intercept=reg[\"intercept\"], color=\"orchid\")\n",
    "    + pn.geom_point(color=\"white\", fill=\"yellowgreen\", size=4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88485b34-d752-49d5-b9fc-dd8ac5d49915",
   "metadata": {},
   "source": [
    "## Regression with pytorch\n",
    "\n",
    "By reusing and adapting the code used in the previous notebook for a\n",
    "simple regression without intercept, we can try to reproduce the\n",
    "`scikit-learn` computation using pytorch.\n",
    "\n",
    "After converting our input data (`temperature`) and target values\n",
    "(`icecream`) to tensors, we will initialize our two parameters: the\n",
    "weight $w$ (the slope) and the bias $b$ (the intercept) both with a\n",
    "default value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert x and y data to tensors\n",
    "x = torch.tensor(temperature)\n",
    "y = torch.tensor(icecream)\n",
    "\n",
    "# Slope (weight) and intercept (bias) parameter\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29299f1f-2e47-4457-b42e-94ef584136c2",
   "metadata": {},
   "source": [
    "Our `forward` function applies our new model, $y = w \\times x + b$. We\n",
    "use the mean squared error as loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and prediction function\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889c53ef-f7ff-4688-a52c-cfd390275526",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "Write the code to run the training process. This code will be quite\n",
    "similar to the one we used before, except there is now two parameters to\n",
    "adjust at each step instead of one.\n",
    "\n",
    "Run this training process for 20 epochs with a step size of 0.001.\n",
    "\n",
    "For now on we will use a predefined function for our training process to\n",
    "keep track of the different loss, gradient and parameter values at each\n",
    "training step.\n",
    "\n",
    "With a step size of 0.001, we see that the weight of our model (the\n",
    "slope of the regression line) goes up in the first epochs, then starts\n",
    "to go down very slowly. The bias goes up, but also very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {\"x\": x, \"y\": y, \"w_init\": 0.0, \"b_init\": 0.0}\n",
    "model_2p.train(step_size=0.001, epochs=20, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eaa1ea-c78d-44d8-958b-1e8d78d7d955",
   "metadata": {},
   "source": [
    "If we increase the step size a bit to 0.002, the loss goes down a bit\n",
    "faster, but the weight oscillates around the optimal value during the\n",
    "first epochs. The bias still goes up very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(step_size=0.002, epochs=10, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebad70e9-80b1-4481-b9db-460835b4f7c4",
   "metadata": {},
   "source": [
    "If we increase the step size to 0.003, the loss goes down a bit more\n",
    "slowly and regularly, but the weight value oscillates greatly around its\n",
    "optimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(step_size=0.003, epochs=10, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb3ee93-b89f-41b5-8d61-84801af08251",
   "metadata": {},
   "source": [
    "If we increase again the step size to 0.004, the loss doesn’t go down\n",
    "anymore and the training process becomes divergent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(step_size=0.004, epochs=10, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e17221-1e35-458e-9323-f666d4541dc8",
   "metadata": {},
   "source": [
    "### Graphical representations\n",
    "\n",
    "To try to understand why the training process doesn’t seem to be able to\n",
    "reach the optimum weight and bias values, we can try to represent the\n",
    "loss graphically.\n",
    "\n",
    "In the following plot, the space of values of weight $w$ and bias $b$ is\n",
    "split into a grid. At each grid point, the loss value is plotted as a\n",
    "circle with a varying radius. The gradient value of the loss function at\n",
    "each point is represented as a red arrow: its orientation gives the\n",
    "“direction” the parameters must be modified in order for the loss value\n",
    "to increase as much as possible, and its length is proportional to the\n",
    "intensity of this increase. Thus, if we want for our loss value to\n",
    "decrease, we must follow the opposite direction given by these arrows.\n",
    "\n",
    "The blue dot in the center is the optimal parameters values, *ie* the\n",
    "values of $w$ and $b$ for which the loss is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params = {\n",
    "    \"x\": x,\n",
    "    \"y\": y,\n",
    "    \"true_weight\": reg[\"slope\"],\n",
    "    \"true_bias\": reg[\"intercept\"],\n",
    "    \"grad_scale\": 6000,\n",
    "}\n",
    "model_2p.plot_loss(**graphic_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37a4461-0ef4-46f2-bf6f-766fa351b79e",
   "metadata": {},
   "source": [
    "We can see that the gradients are almost all “horizontal”. This is due\n",
    "to the fact that our two parameters do not have the same scale: a\n",
    "variation of 1 of $w$ (the slope) will have a much higher effect on the\n",
    "loss value than a variation of 1 of $b$ (the intercept).\n",
    "\n",
    "We can try to visualise what this means for the training process.\n",
    "\n",
    "In the next plot, we represent a training process of 10 epochs with a\n",
    "step size of 0.001 starting at $w = 2$ and $b = 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params.update({\"w_init\": 2.0, \"b_init\": 50.0})\n",
    "model_2p.plot_train(\n",
    "    step_size=0.001,\n",
    "    epochs=10,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3749faee-5c1c-4c8a-a807-2979b7003892",
   "metadata": {},
   "source": [
    "We see that the gradient descent seems to go only horizontally, slowing\n",
    "down rapidly after the first epochs.\n",
    "\n",
    "If we increase the number of epochs, we see that after a while going\n",
    "horizontally, the gradient descent starts to “turn” into the direction\n",
    "of the optimum value (but still very slowly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.001,\n",
    "    epochs=200,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618be62-47c8-4f10-a258-d9740578b0c1",
   "metadata": {},
   "source": [
    "We have to increase the number of epochs a lot to see the training\n",
    "process getting very close to the optimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.001,\n",
    "    epochs=3000,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356c57a-892e-42ad-8239-8f77942d6b53",
   "metadata": {},
   "source": [
    "If we increase the step size to 0.003, we can see that the horizontal\n",
    "gradient descent is more “chaotic”. However the training process gets\n",
    "close to the optimum a bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.003,\n",
    "    epochs=1000,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f2cd2-6f7f-49f6-b349-d797d79b38d9",
   "metadata": {},
   "source": [
    "Finally if we increase the step size further to 0.004, we see that the\n",
    "training process immediately starts to diverge for the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.004,\n",
    "    epochs=10,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18262aea-0997-4535-8311-0f66a5b89719",
   "metadata": {},
   "source": [
    "## Regression with pytorch on transformed data\n",
    "\n",
    "One way to improve our training process is to transform our original\n",
    "data so that weight and bias will be on a more similar “scale”.\n",
    "\n",
    "### Normalized data\n",
    "\n",
    "First we will try to standardize the temperature values to be between 0\n",
    "and 1 by applying a `minmax_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "temp_n = preprocessing.minmax_scale(np.array(temperature))\n",
    "x_n = torch.tensor(temp_n, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49d7d10-0d4f-435f-bd17-c7f2b4091444",
   "metadata": {},
   "source": [
    "We can compute the new optimum weight and bias values with\n",
    "`scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_n = skl_regression(temp_n, icecream)\n",
    "print(f\"slope: {reg_n['slope']:.2f}, intercept: {reg_n['intercept']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb828fb-c3e4-4bac-ab6c-ae7f04b4cc83",
   "metadata": {},
   "source": [
    "If we run our pytorch implementation on this transformed data, we can\n",
    "see that with a large step size, the training process seems to start to\n",
    "converge towards the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(x_n, y, step_size=0.4, epochs=20, w_init=0.0, b_init=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71460372-3a29-4cb2-a2c5-68be6c1f57e2",
   "metadata": {},
   "source": [
    "If we plot the loss at different points, we can see that the values and\n",
    "the gradient orientations are quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_n = {\n",
    "    \"x\": x_n,\n",
    "    \"y\": y,\n",
    "    \"true_weight\": reg_n[\"slope\"],\n",
    "    \"true_bias\": reg_n[\"intercept\"],\n",
    "    \"grad_scale\": 5,\n",
    "    \"b_factor\": 4,\n",
    "}\n",
    "model_2p.plot_loss(**graphic_params_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0883fdc1-1ea1-441d-b4e5-8240b276fc38",
   "metadata": {},
   "source": [
    "If we add the visualization of a training process with a step size of\n",
    "0.4, we can see that the process converges much faster towards the\n",
    "optimal value, which is reached in about 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_n.update({\"w_init\": 0.0, \"b_init\": 0.0, \"w_factor\": 1.0})\n",
    "model_2p.plot_train(**graphic_params_n, step_size=0.4, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d5b3c0-7dc3-4df0-9931-4bc487484835",
   "metadata": {},
   "source": [
    "A smaller step size of 0.1 is slower but still reaches the optimum in\n",
    "about 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_n, step_size=0.1, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf6c10-d7eb-4d1f-88b1-acd8aca367e0",
   "metadata": {},
   "source": [
    "With a step size of 0.75, the training process converges even faster. We\n",
    "can see that the gradient descent is less smooth as it “oscillates”\n",
    "between two gradient directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_n, step_size=0.75, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c97e45-52f0-4ac8-aea6-52bd54079d2b",
   "metadata": {},
   "source": [
    "Finally, when the step size is too high, the training process starts\n",
    "diverging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_n, step_size=0.9, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca31f8-5ffd-4ddf-8433-0ad564d7e28b",
   "metadata": {},
   "source": [
    "### Scaled data\n",
    "\n",
    "Another possible transformation of the input data is to scale it by\n",
    "substracting its mean and dividing by its standard deviation. This can\n",
    "be done easily using `scikit-learn`’s `scale` preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "temp_s = preprocessing.scale(temperature, with_mean=True)\n",
    "x_s = torch.tensor(temp_s, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f48a07d-dbd6-406e-aac8-c1d7cc264780",
   "metadata": {},
   "source": [
    "We can again compute the new optimal weight and bias values with\n",
    "`scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_s = skl_regression(temp_s, icecream)\n",
    "print(f\"slope: {reg_s['slope']:.2f}, intercept: {reg_s['intercept']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3925800c-c40f-4598-baa8-3c8063299062",
   "metadata": {},
   "source": [
    "If we run our pytorch implementation on this standardize data, we can\n",
    "see that with a large step size, the training is able to converge\n",
    "towards the true values quite rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(x=x_s, y=y, step_size=0.3, epochs=10, w_init=0.0, b_init=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b7075-1d48-4a06-b206-882593e48ea4",
   "metadata": {},
   "source": [
    "We can once again try to visualize the loss gradients and values along a\n",
    "grid of $w$ and $b$ values. We see that each gradient seems to point\n",
    "directly to the opposite direction of the optimum.\n",
    "\n",
    "This is because with scaled input data, $w$ and $b$ have the same\n",
    "“effect”: increasing $w$ value by one is equivalent to increasing $b$ by\n",
    "one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_s = {\n",
    "    \"x\": x_s,\n",
    "    \"y\": y,\n",
    "    \"true_weight\": reg_s[\"slope\"],\n",
    "    \"true_bias\": reg_s[\"intercept\"],\n",
    "    \"grad_scale\": 15,\n",
    "}\n",
    "model_2p.plot_loss(**graphic_params_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fdc88-2e97-4357-9f4b-a85331976d87",
   "metadata": {},
   "source": [
    "We can plot the training process with a step size of 0.3. The gradient\n",
    "descent seems to be straightforward and goes directly to the optimum\n",
    "value, which is reached within less than 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_s.update({\"w_init\": 0.0, \"b_init\": 0.0, \"w_factor\": 1.0, \"b_factor\": 1.0})\n",
    "model_2p.plot_train(**graphic_params_s, step_size=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65de9b2-68c6-4137-b116-6a52349d3920",
   "metadata": {},
   "source": [
    "With a larger step size of 0.6, the gradient descent first “overshoots”\n",
    "the optimum values, but it then rapidly converges towards it in a few\n",
    "epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_s, step_size=0.6, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c6cd06-eec6-42f5-bd0b-aa1d0947481d",
   "metadata": {},
   "source": [
    "And, as before, if the step size is too high the training process starts\n",
    "to diverge, oscillating farther and farther from the optimum instead of\n",
    "converging towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_s, step_size=1.0, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
