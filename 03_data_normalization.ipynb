{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb5179a-614c-434f-9885-09a7d28a2d7f",
   "metadata": {},
   "source": [
    "# Data normalization\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "``` shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as pn\n",
    "import polars as pl\n",
    "import torch\n",
    "\n",
    "from adl import model_2p\n",
    "from adl.sklearn import skl_regression\n",
    "\n",
    "pl.Config(tbl_rows=30, float_precision=2)\n",
    "pn.theme_set(pn.theme_minimal() + pn.theme(plot_background=pn.element_rect(fill=\"white\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8fb459-4cd6-4f51-9298-90ca5e4f70b9",
   "metadata": {},
   "source": [
    "In this notebook we will take a look at a slightly more complicated\n",
    "model with two parameters, and at the effect of data normalization on\n",
    "the training process.\n",
    "\n",
    "## Data\n",
    "\n",
    "We first create a small nonsensical dataset with two numerical\n",
    "variables: the temperature and the quantity of ice cream sold at a shop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=temperature, y=icecream))\n",
    "    + pn.geom_hline(yintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_vline(xintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_point(color=\"white\", fill=\"yellowgreen\", size=4)\n",
    "    + pn.labs(x=\"temperature\", y=\"icecream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827491ac-810d-4f39-a546-8f6387e1136c",
   "metadata": {},
   "source": [
    "This time we will try to predict the `icecream` values from the\n",
    "`temperature` values with a simple linear model with both a slope and an\n",
    "intercept: $y = w \\times x + b$. Our model now has two parameters, a\n",
    "**weight** $w$, and a **bias** $b$.\n",
    "\n",
    "## Regression with scikit-learn\n",
    "\n",
    "As a reference, we first compute the “real” optimal slope and intercept\n",
    "values with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = skl_regression(x=temperature, y=icecream, fit_intercept=True)\n",
    "print(f\"slope: {reg['slope']:.2f}, intercept: {reg['intercept']:.2f}, mse: {reg['mse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=temperature, y=icecream))\n",
    "    + pn.geom_hline(yintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_vline(xintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_abline(slope=reg[\"slope\"], intercept=reg[\"intercept\"], color=\"orchid\")\n",
    "    + pn.geom_point(color=\"white\", fill=\"yellowgreen\", size=4)\n",
    "    + pn.labs(x=\"temperature\", y=\"icecream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe778bd7-6556-47c1-8d30-f50b6705204f",
   "metadata": {},
   "source": [
    "## Regression with pytorch\n",
    "\n",
    "As we did in the previous notebook, we can do the same computation using\n",
    "pytorch to search for the $w$ and $b$ values that would minimize the\n",
    "mean squared error of our model.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "Using pytorch, write and run the code implementing the training process\n",
    "to find the values of $w$ and $b$ which minimize the mean square error\n",
    "loss between true and predicted values.\n",
    "\n",
    "The code will be quite similar as the one in the previous notebook,\n",
    "except that we now have two parameters to adjust at each step.\n",
    "\n",
    "1.  create the input, target and parameters tensors\n",
    "2.  create a `forward()` method which applies our model to input data\n",
    "    passed as argument\n",
    "3.  create a loss function using one of pytorch predefined methods\n",
    "4.  implement a training process using a `for` loop\n",
    "\n",
    "Run this training process for 20 epochs with a step size of 0.001. Print\n",
    "the epoch, loss, $w$ and $b$ values at each step.\n",
    "\n",
    "For now on we will use a predefined function for our training process to\n",
    "keep track of the different loss, gradient and parameter values at each\n",
    "training step.\n",
    "\n",
    "With a step size of 0.001, we see that the weight of our model (the\n",
    "slope of the regression line) goes up in the first epochs, then starts\n",
    "to go down very slowly. The bias goes up, but also very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert x and y data to tensors\n",
    "x = torch.tensor(temperature)\n",
    "y = torch.tensor(icecream)\n",
    "\n",
    "train_params = {\"x\": x, \"y\": y, \"w_init\": 0.0, \"b_init\": 0.0}\n",
    "model_2p.train(step_size=0.001, epochs=20, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d9d59d-90ed-431d-884a-f6571d1558fe",
   "metadata": {},
   "source": [
    "If we increase the step size a bit to 0.002, the loss goes down a bit\n",
    "faster, but the weight oscillates around the optimal value during the\n",
    "first epochs. The bias still goes up very slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(step_size=0.002, epochs=10, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae2d1d0-c297-42d3-aa69-ece3529464a6",
   "metadata": {},
   "source": [
    "If we increase the step size to 0.003, the loss goes down a bit more\n",
    "slowly and regularly, but the weight value oscillates greatly around its\n",
    "optimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(step_size=0.003, epochs=10, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65fe308-f64c-47eb-a279-24854bd04110",
   "metadata": {},
   "source": [
    "If we increase again the step size to 0.004, the loss doesn’t go down\n",
    "anymore and the training process becomes divergent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.train(step_size=0.004, epochs=10, **train_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebd9c1-d62e-4c9e-b569-a2fa028b1015",
   "metadata": {},
   "source": [
    "### Graphical representations\n",
    "\n",
    "To try to understand why the training process doesn’t seem to be able to\n",
    "reach the optimum weight and bias values, we can try to represent the\n",
    "loss graphically.\n",
    "\n",
    "In the plot below, the space of possible values for weight $w$ and bias\n",
    "$b$ is divided into a grid. At each grid point, the loss value is\n",
    "represented as a circle with a varying radius. The gradient of the loss\n",
    "function at each point is represented as a red arrow: its orientation\n",
    "gives the “direction” the parameters must be modified in order for the\n",
    "loss value to increase as much as possible, and its length is\n",
    "proportional to the magnitude of this increase. Thus, if we want for our\n",
    "loss value to decrease, we must follow the opposite direction given by\n",
    "these arrows.\n",
    "\n",
    "The blue dot in the center is the optimal parameters values, *ie* the\n",
    "values of $w$ and $b$ for which the loss is minimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params = {\n",
    "    \"x\": x,\n",
    "    \"y\": y,\n",
    "    \"true_weight\": reg[\"slope\"],\n",
    "    \"true_bias\": reg[\"intercept\"],\n",
    "    \"grad_scale\": 6000,\n",
    "}\n",
    "model_2p.plot_loss(**graphic_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92034cf5-656a-46e1-9676-4410293b1dc9",
   "metadata": {},
   "source": [
    "We can see that the gradients are almost all “horizontal”. This is due\n",
    "to the fact that our two parameters do not have the same scale: a\n",
    "variation of 1 on $w$ (the slope) will have an higher effect on the loss\n",
    "value than a variation of 1 on $b$ (the intercept).\n",
    "\n",
    "We can try to visualise what this means for the training process.\n",
    "\n",
    "In the next plot, we represent a training process of 10 epochs with a\n",
    "step size of 0.001 starting at $w = 2$ and $b = 50$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params.update({\"w_init\": 2.0, \"b_init\": 50.0})\n",
    "model_2p.plot_train(\n",
    "    step_size=0.001,\n",
    "    epochs=10,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c274e13-61f3-499f-9332-389a9475bd99",
   "metadata": {},
   "source": [
    "We see that the gradient descent seems to go only horizontally, slowing\n",
    "down rapidly after the first epochs.\n",
    "\n",
    "If we increase the number of epochs, we see that after a while going\n",
    "horizontally, the gradient descent starts to “turn” into the direction\n",
    "of the optimum value (but still very slowly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.001,\n",
    "    epochs=200,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4311c-fd6c-495b-b742-24c1c0ebf1b1",
   "metadata": {},
   "source": [
    "We have to increase the number of epochs a lot to see the training\n",
    "process getting very close to the optimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.001,\n",
    "    epochs=3000,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e26ae1c-67ed-43ec-a81a-484b3b244f5b",
   "metadata": {},
   "source": [
    "If we increase the step size to 0.003, we can see that the horizontal\n",
    "gradient descent is more “chaotic”. However the training process gets\n",
    "close to the optimum a bit faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.003,\n",
    "    epochs=1000,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5289acf7-830b-48b2-9946-50b7b8899ed1",
   "metadata": {},
   "source": [
    "Finally if we increase the step size further to 0.004, we see that the\n",
    "training process immediately starts to diverge for the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(\n",
    "    step_size=0.004,\n",
    "    epochs=10,\n",
    "    **graphic_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b86740-7c9d-4e13-9b53-cb3790e9ee0b",
   "metadata": {},
   "source": [
    "## Regression with pytorch on transformed data\n",
    "\n",
    "One way to improve our training process is to transform our original\n",
    "data so that weight and bias will be on a more similar “scale”.\n",
    "\n",
    "### Normalized data\n",
    "\n",
    "First we will try to standardize the temperature values to be between 0\n",
    "and 1 by applying scikit-learn’s `preprocessing.minmax_scale`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "temp_n = preprocessing.minmax_scale(temperature)  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=temp_n, y=icecream))\n",
    "    + pn.geom_hline(yintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_vline(xintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_point(color=\"white\", fill=\"yellowgreen\", size=4)\n",
    "    + pn.labs(x=\"temp_n\", y=\"icecream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22da70a-c622-4bc6-a536-7d57431f7cc0",
   "metadata": {},
   "source": [
    "We can compute the new optimum weight and bias values with\n",
    "`scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_n = skl_regression(temp_n, icecream)\n",
    "print(f\"slope: {reg_n['slope']:.2f}, intercept: {reg_n['intercept']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c258829-0e2e-42a5-80e0-d33a93070c66",
   "metadata": {},
   "source": [
    "If we run our pytorch implementation on this transformed data, we can\n",
    "see that with a large step size, the training process seems to start to\n",
    "converge towards the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_n = torch.tensor(temp_n, dtype=torch.float)\n",
    "model_2p.train(x_n, y, step_size=0.4, epochs=20, w_init=0.0, b_init=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6861c4-59bf-45d7-963c-3c0e361290fc",
   "metadata": {},
   "source": [
    "If we plot the loss at different points, we can see that the values and\n",
    "the gradient orientations are quite different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_n = {\n",
    "    \"x\": x_n,\n",
    "    \"y\": y,\n",
    "    \"true_weight\": reg_n[\"slope\"],\n",
    "    \"true_bias\": reg_n[\"intercept\"],\n",
    "    \"grad_scale\": 5,\n",
    "    \"b_factor\": 4,\n",
    "}\n",
    "model_2p.plot_loss(**graphic_params_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da458736-4556-4071-8ceb-b38f6fc07bd1",
   "metadata": {},
   "source": [
    "If we add the visualization of a training process with a step size of\n",
    "0.4, we can see that the process converges much faster towards the\n",
    "optimal value, which is reached in about 50 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_n.update({\"w_init\": 0.0, \"b_init\": 0.0, \"w_factor\": 1.0})\n",
    "model_2p.plot_train(**graphic_params_n, step_size=0.4, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9eb6d9-9600-4d28-94a2-c8390504efa2",
   "metadata": {},
   "source": [
    "A smaller step size of 0.1 is slower but still reaches the optimum in\n",
    "about 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_n, step_size=0.1, epochs=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40159154-9436-41b8-9f75-59b697cabdf2",
   "metadata": {},
   "source": [
    "With a step size of 0.75, the training process converges even faster. We\n",
    "can see that the gradient descent is less smooth as it “oscillates”\n",
    "between two gradient directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_n, step_size=0.75, epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c13065-1c43-4fcc-96d2-eda67847e78c",
   "metadata": {},
   "source": [
    "Finally, when the step size is too high, the training process starts\n",
    "diverging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_n, step_size=0.9, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea557b-64ab-4d4d-be70-0209c9da365c",
   "metadata": {},
   "source": [
    "### Scaled data\n",
    "\n",
    "Another possible transformation of the input data is to scale it by\n",
    "substracting its mean and dividing by its standard deviation. This can\n",
    "be done easily using `scikit-learn`’s `scale` preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "temp_s = preprocessing.scale(temperature, with_mean=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=temp_s, y=icecream))\n",
    "    + pn.geom_hline(yintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_vline(xintercept=0, linetype=\"dotted\")\n",
    "    + pn.geom_point(color=\"white\", fill=\"yellowgreen\", size=4)\n",
    "    + pn.labs(x=\"temp_s\", y=\"icecream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7bd40-8805-4a01-b8bc-801fccd60a30",
   "metadata": {},
   "source": [
    "We can again compute the new optimal weight and bias values with\n",
    "`scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_s = skl_regression(temp_s, icecream)\n",
    "print(f\"slope: {reg_s['slope']:.2f}, intercept: {reg_s['intercept']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d52e47-3c1e-4c06-834d-bfd204814ffc",
   "metadata": {},
   "source": [
    "If we run our pytorch implementation on this scaled data, we can see\n",
    "that with a large step size, the training is able to converge towards\n",
    "the true values quite rapidly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_s = torch.tensor(temp_s, dtype=torch.float)\n",
    "model_2p.train(x=x_s, y=y, step_size=0.3, epochs=10, w_init=0.0, b_init=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01359978-7cfd-4b0f-9995-30c01a2b8176",
   "metadata": {},
   "source": [
    "We can once again try to visualize the loss gradients and values along a\n",
    "grid of $w$ and $b$ values. We see that the contour of our loss seems\n",
    "more circular, and the gradient seem to point directly to the opposite\n",
    "direction of the optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_s = {\n",
    "    \"x\": x_s,\n",
    "    \"y\": y,\n",
    "    \"true_weight\": reg_s[\"slope\"],\n",
    "    \"true_bias\": reg_s[\"intercept\"],\n",
    "    \"grad_scale\": 15,\n",
    "}\n",
    "model_2p.plot_loss(**graphic_params_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1802e7ad-b8c3-4d55-a736-1285183cabe8",
   "metadata": {},
   "source": [
    "We can plot the training process with a step size of 0.3. The gradient\n",
    "descent seems to be straightforward and goes directly to the optimum\n",
    "value, which is reached within less than 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphic_params_s.update({\"w_init\": 0.0, \"b_init\": 0.0, \"w_factor\": 1.0, \"b_factor\": 1.0})\n",
    "model_2p.plot_train(**graphic_params_s, step_size=0.3, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb530dc-91a0-4b25-9b30-107af06dd6f7",
   "metadata": {},
   "source": [
    "With a larger step size of 0.6, the gradient descent first “overshoots”\n",
    "the optimum values, but it then rapidly converges towards it in a few\n",
    "epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_s, step_size=0.6, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7efdad-5b68-4b7e-bb03-a9b69b6f6e56",
   "metadata": {},
   "source": [
    "And, as before, if the step size is too high the training process starts\n",
    "to diverge, oscillating farther and farther from the optimum instead of\n",
    "converging towards it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2p.plot_train(**graphic_params_s, step_size=1.0, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
