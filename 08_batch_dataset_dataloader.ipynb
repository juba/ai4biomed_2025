{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03f5886d-a0f0-4968-911b-668e0d1290ef",
   "metadata": {},
   "source": [
    "# Batches, datasets and data loaders\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "``` shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import requests\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from adl.sklearn import skl_regression\n",
    "from adl import cooking, model_1p\n",
    "\n",
    "pn.theme_set(pn.theme_minimal() + pn.theme(plot_background=pn.element_rect(fill=\"white\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc403e-d4c6-4c20-97f8-c6e86bfb0161",
   "metadata": {},
   "source": [
    "Up to this point we have only worked on very small toy datasets, but in\n",
    "real-world deep learning applications the datasets can be huge, either\n",
    "because we have a very large number of data points, and/or because each\n",
    "data point is itself quite big (like sequences, images or videos). In\n",
    "this case it is impossible to apply a training step (forward pass and\n",
    "backpropagation) to the whole dataset, due to memory limitations.\n",
    "\n",
    "To overcome this, the training steps will rather be applied to\n",
    "*mini-batches* of data:\n",
    "\n",
    "-   the dataset is divided into smaller subsets of data points of a\n",
    "    given size. Each subset is called a *mini-batch*, or a *batch*.\n",
    "-   the train step will be applied sequentially to each batch: the\n",
    "    forward pass, backpropagation and parameters adjustment will be\n",
    "    performed for each batch, one after the other.\n",
    "-   an *epoch* is reached when all the batches have been processed and\n",
    "    the entire training dataset has been seen by the network.\n",
    "\n",
    "To demonstrate this we will reuse cake recipe rating example from the\n",
    "*overfitting* notebook where ratings were predicted based on cooking\n",
    "time. But this time we will generate a much larger dataset of 500 000\n",
    "data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "time, score = cooking.generate_data(size=500_000, noise_scale=0.9)\n",
    "\n",
    "cooking.scatter_plot(time, score, size=1, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0938373b-0098-4648-a328-8bd567ed3995",
   "metadata": {},
   "source": [
    "We define a regression network class and model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=10),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=10, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Center data\n",
    "        x = x - 115 / 2\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "model = RegressionNetwork()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e039e33-3dd4-4c83-a382-81fb77aa1990",
   "metadata": {},
   "source": [
    "The train step will be the same as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training step\n",
    "def train_step(x, y, model, loss_fn, optimizer):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted values\n",
    "    y_pred = model(x)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagations\n",
    "    loss.backward()\n",
    "    # Parameters adjustment\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05e6eb-89a0-42dc-bc87-45e140c05189",
   "metadata": {},
   "source": [
    "The difference with the previous notebook is the way we will run our\n",
    "training steps.\n",
    "\n",
    "Previously, we applied it to the whole dataset at once for each epoch:\n",
    "\n",
    "``` py\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    loss = train_step(time, score, model, loss_fn, optimizer)\n",
    "    if (epoch + 1) % 500 == 0:\n",
    "        print(f\"Epoch {epoch + 1:2} - loss: {loss:5.3f}\")\n",
    "```\n",
    "\n",
    "This time we will introduce a second loop which will iterate through\n",
    "batches of data points. The train step (loss computation and parameters\n",
    "adjustment) is applied to each batch after another. At the end of each\n",
    "epoch we compute the average batch loss as the global epoch loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "\n",
    "epochs = 10\n",
    "# Batch size\n",
    "batch_size = 10_000\n",
    "# Number of batches\n",
    "n_batches = len(time) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # For each batch, extract the corresponding x and y data\n",
    "        x_batch = time[batch * batch_size : (batch + 1) * batch_size - 1]\n",
    "        y_batch = score[batch * batch_size : (batch + 1) * batch_size - 1]\n",
    "        # Compute loss on this batch\n",
    "        batch_loss = train_step(x_batch, y_batch, model, loss_fn, optimizer)\n",
    "        # Accumulate loss between batches\n",
    "        loss += batch_loss.item()\n",
    "    # Compute average loss for this epoch\n",
    "    loss /= n_batches\n",
    "    print(f\"Epoch {epoch + 1:2} - loss: {loss:5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab5f2f-b13a-490c-9fc3-4c89375dc9ed",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders\n",
    "\n",
    "In the previous example we performed the batches extraction manually,\n",
    "but in practice it can quickly become complex and cumbersome. Pytorch\n",
    "provides two tools to make batch processing a bit easier: `Dataset` and\n",
    "`DataLoader`.\n",
    "\n",
    "-   A `Dataset` object describes a data source, its size and the way to\n",
    "    get an item from it. It allows to access data from a numpy array, a\n",
    "    torch tensor, a file, or any other resource accessible via Python\n",
    "    code.\n",
    "-   A `Dataloader` object allows to load data from a `Dataset` while\n",
    "    handling features like batch loading and shuffling.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "To demonstrate the use of `Dataset`, we first generate two sample\n",
    "datasets, one for training and one for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_train, score_train = cooking.generate_data(size=500_000, noise_scale=0.9)\n",
    "time_valid, score_valid = cooking.generate_data(size=18_000, noise_scale=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d6f09-c915-4164-bb16-c7783ad67cbd",
   "metadata": {},
   "source": [
    "The next step is to define a `Dataset` class for our data. This is a\n",
    "Python class which inherits from\n",
    "[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset),\n",
    "and must implement three methods:\n",
    "\n",
    "-   `__init__()`, the class constructor\n",
    "-   `__len__()`, which must return the length of our dataset (the number\n",
    "    of data points)\n",
    "-   `__getitem__()`, which, given an integer index as argument, must\n",
    "    return a couple of `(data, label)` corresponding to this index.\n",
    "\n",
    "Here we will create a class called `RegressionDataset`:\n",
    "\n",
    "-   the constructor takes two `time` and `score` tensors as arguments\n",
    "    and store them as attributes. - the `__len__()` method returns the\n",
    "    length of these tensors.\n",
    "-   the `__getitem__()` method returns a tuple of the time and score\n",
    "    values at the given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, time, score):\n",
    "        # Check if both tensors have the same length\n",
    "        if len(time) != len(score):\n",
    "            msg = \"time and score don't have the same length\"\n",
    "            raise ValueError(msg)\n",
    "        # Store time and score as attributes\n",
    "        self.time = time\n",
    "        self.score = score\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of data points\n",
    "        return len(self.score)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Returns the time and score values for the given index\n",
    "        time_index = self.time[index]\n",
    "        score_index = self.score[index]\n",
    "        return time_index, score_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8491e90c-7990-483c-93f9-c5eb76438ae7",
   "metadata": {},
   "source": [
    "Our `Dataset` is quite simple here as it just stores and retrieve values\n",
    "from tensors, but it could be more complex. For example the constructor\n",
    "could get a list of filenames containing images and their corresponding\n",
    "labels, and the `__getitem__` method would then open and read the files\n",
    "and preprocess the image data.\n",
    "\n",
    "Now that our `RegressionDataset` class is defined, we can create two\n",
    "training and validation dataset objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RegressionDataset(time_train, score_train)\n",
    "valid_dataset = RegressionDataset(time_valid, score_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f18d65-e78e-4c0d-b3df-38e49f0ea402",
   "metadata": {},
   "source": [
    "We can apply `len` to one of these objects to get the number of its data\n",
    "points, or index it to get a (data, label) tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002e4e36-2a96-4328-9aab-001da50b4a50",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "Once our `Dataset` objects are defined, we can create associated\n",
    "`Dataloader` objects, which will handle the batches extraction and\n",
    "traversal.\n",
    "\n",
    "For this, we will create\n",
    "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "instances by passing them (among other possible arguments):\n",
    "\n",
    "-   a `Dataset` object\n",
    "-   the batch size\n",
    "-   a `shuffle` argument: if `True`, the data points will be reshuffled\n",
    "    randomly before each epoch. This means that batches will be\n",
    "    different from one epoch to another.\n",
    "\n",
    "We create two training and validation loaders with a batch size of 10\n",
    "000. The training loader is shuffled, to have different batches at each\n",
    "epoch. This is not useful for the validation loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10_000\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74bb01b-da55-4e3e-a0ab-8b142e529449",
   "metadata": {},
   "source": [
    "Once created, we can iterate over a `Dataloader`. Each iteration will\n",
    "return a batch of `(time, score)` data.\n",
    "\n",
    "For example, we can iterate over our validation loader. This yields two\n",
    "batches, the first with the wanted size of 10 000, and the second one\n",
    "with a size of 8 000 (as there are only 18 000 points in our validation\n",
    "dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_loader:\n",
    "    time, score = batch\n",
    "    print(time.shape, score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297db31b-7dc3-4b8a-8f2d-1c70a8a3dcca",
   "metadata": {},
   "source": [
    "If we iterate again, we will start a new epoch and get the same batches\n",
    "again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_loader:\n",
    "    time, score = batch\n",
    "    print(time.shape, score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60f088-eb86-444f-a23d-47e9a290a65c",
   "metadata": {},
   "source": [
    "We can rewrite our training code using our data loaders. Inside each\n",
    "epoch, we first iterate through our `train_loader` object and run a\n",
    "training step on the yielded batch. Then, once all training batches have\n",
    "been processed, we iterate through our `valid_loader`, this time to\n",
    "compute the validation loss for this epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionNetwork()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)  # type: ignore\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Set model in train mode\n",
    "        model.train()\n",
    "        # Run a training step and accumulate loss value\n",
    "        batch_loss = train_step(x_batch, y_batch, model, loss_fn, optimizer)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "    # Compute average training loss for this epoch\n",
    "    loss /= len(train_loader)\n",
    "\n",
    "    # Iterate through validation batches\n",
    "    valid_loss = 0\n",
    "    for x_valid_batch, y_valid_batch in valid_loader:\n",
    "        # Set model in evaluation (inference) mode\n",
    "        model.eval()\n",
    "        # Compute and accumulate the batch loss\n",
    "        y_valid_pred = model(x_valid_batch)\n",
    "        valid_batch_loss = loss_fn(y_valid_pred, y_valid_batch)\n",
    "        valid_loss += valid_batch_loss\n",
    "\n",
    "    # Compute the average validation loss for this epoch\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print(f\"{epoch + 1:5}. loss: {loss:5.3f}, valid_loss: {valid_loss:5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18401843-9b71-410f-8bb0-e1e3e991fed1",
   "metadata": {},
   "source": [
    "The training is working well, even if it is slower than previously.\n",
    "However, the slowdown would be smaller if the cost of the\n",
    "`__getitem__()` operation was higher (if, for example, we were reading a\n",
    "file). And we have some nice bonus features, such as managing\n",
    "automatically the size of the last batch, or the shuffling of training\n",
    "data between each epoch.\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "The following python function uses the [Open meteo\n",
    "API](https://open-meteo.com/en/docs) to get different daily weather\n",
    "informations for the past 30 days at the LBBE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data():\n",
    "    url = \"https://api.open-meteo.com/v1/forecast?latitude=45.78&longitude=4.87&daily=precipitation_sum,temperature_2m_mean,wind_speed_10m_mean&past_days=30\"\n",
    "    data = requests.get(url).json()[\"daily\"]  # noqa: S113\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285e612-ee7e-49f7-b4c8-a593893f9f4a",
   "metadata": {},
   "source": [
    "The returned data is a Python dictionary with the following fields:\n",
    "\n",
    "-   `wind_speed_1Om_mean`: mean daily wind speed\n",
    "-   `temperature_2m_mean`: mean daily temperature\n",
    "-   `precipitation_sum`: total daily precipitations\n",
    "\n",
    "1.  Use the `get_weather_data` function to create a new `data` object\n",
    "2.  Create a `WeatherDataset` class that converts wind speed,\n",
    "    temperature and precipitations as tensors and store them as\n",
    "    attributes, and returns temperature and wind speed as input data,\n",
    "    and precipitation as target\n",
    "3.  Create a `weather_dataset` object from your `WeatherDataset` class\n",
    "4.  Create a `DataLoader` object for `weather_dataset` with a\n",
    "    `batch_size` of 10 and no shuffle\n",
    "5.  Iterate over the data loader and print the batch values\n",
    "\n",
    "## Effect of batch size on training process\n",
    "\n",
    "Besides allowing to train a model from bigger datasets, the use of\n",
    "mini-batches also has an effect on the training process itself. This is\n",
    "due to the fact that when using batches, the loss function will be\n",
    "slightly different at each train step (and thus the gradients and the\n",
    "parameters adjustments will also be slightly different).\n",
    "\n",
    "To illustrate this point, we reuse a previous example of linear\n",
    "regression with only one parameter $w$ (the slope of the line). We first\n",
    "generate a random dataset of 500 points of `x` and `y` values where `y`\n",
    "is equal to `x * 2` plus some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 500\n",
    "np.random.seed(1337)\n",
    "# Generate x and y values\n",
    "x = np.random.uniform(low=0, high=10, size=n_points)\n",
    "y = x * 2 + np.random.normal(loc=0, scale=3, size=n_points)\n",
    "\n",
    "# Convert to tensors\n",
    "xt = torch.tensor(x).view(-1, 1)\n",
    "yt = torch.tensor(y).view(-1, 1)\n",
    "\n",
    "# Plot the dataset\n",
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=x, y=y))\n",
    "    + pn.geom_abline(slope=2, intercept=0, color=\"red\")\n",
    "    + pn.geom_point(color=\"royalblue\", size=2, alpha=0.5)\n",
    "    + pn.coord_cartesian(xlim=(0, 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d50cd1-ddcb-4f88-81b1-5c7a93c16a20",
   "metadata": {},
   "source": [
    "We can plot the loss function for our whole dataset, *ie*, the mean\n",
    "squared error of our model $y = w \\times x$ for different values of $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_loss(xt, yt, wmin=-4, wmax=8, gradient=False, ylim=(0, 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6afe73-0964-4865-8068-e5850a61b678",
   "metadata": {},
   "source": [
    "We can see that if $w = 0$ (*ie* with an horizontal regression line),\n",
    "the loss value on our dataset is around 200. As expected, the minimum\n",
    "value of loss is reached for $w$ approximately equal to 2.\n",
    "\n",
    "But what happens if we compute this loss function not over the whole\n",
    "dataset, but only over a subset of it, for example of 32 data points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = random.choices(range(len(x)), k=32)\n",
    "x_subset, y_subset = x[indices], y[indices]\n",
    "reg = skl_regression(x_subset, y_subset, fit_intercept=False)\n",
    "\n",
    "# Plot the dataset\n",
    "(\n",
    "    pn.ggplot()\n",
    "    + pn.geom_abline(slope=2, intercept=0, color=\"red\", alpha=0.2)\n",
    "    + pn.geom_point(mapping=pn.aes(x=x, y=y), color=\"royalblue\", size=2, alpha=0.05)\n",
    "    + pn.geom_abline(slope=reg[\"slope\"], intercept=0, color=\"red\")\n",
    "    + pn.geom_point(mapping=pn.aes(x=x_subset, y=y_subset), color=\"royalblue\", size=2, alpha=0.9)\n",
    "    + pn.coord_cartesian(xlim=(0, 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bd7da4-8598-4aa8-8f5d-4bf927beb778",
   "metadata": {},
   "source": [
    "If we take a subset of our data, we can see that the slope of the\n",
    "regression line is slightly different. So we can suppose that the\n",
    "overall loss function values will be different too.\n",
    "\n",
    "We can visualize this by randomly sampling 32 data points, plotting the\n",
    "associated loss function (in grey on the plot below) and compare it with\n",
    "the loss function of our whole data (the dashed red line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=32, n_batches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05317a-263b-4d7e-a79a-681551cb9e7a",
   "metadata": {},
   "source": [
    "We can see that the loss of our batch is not identical to the “full”\n",
    "loss. It has about the same shape but its values are not the same.\n",
    "\n",
    "We can guess that each batch of 32 points will generate a different loss\n",
    "function. We can visualize this variability by generating many batches\n",
    "and plotting their losses on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=32, n_batches=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5540d4e-1abf-4925-b270-3d698c0ab172",
   "metadata": {},
   "source": [
    "What happens if we decrease the batch size?\n",
    "\n",
    "If we create batches of 16 data points, we can see that the variability\n",
    "around the “full” loss is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=16, n_batches=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee7b2c-3f2e-4ee2-bdcd-0f9180219032",
   "metadata": {},
   "source": [
    "As an extreme example, with a batch size of 1, the loss function is\n",
    "calculated for only one data point. The variability of the loss is then\n",
    "maximal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=1, n_batches=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e259d9f3-dac2-4d77-a39f-6d14771de744",
   "metadata": {},
   "source": [
    "On the contrary, with a larger batch size of 256, the loss functions\n",
    "will more closely approximate the “full” loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=256, n_batches=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5f0f8-af7c-44f8-ae26-eef95f4cd2ed",
   "metadata": {},
   "source": [
    "Here is another example with a more complex loss function, still with a\n",
    "unique parameter $w$. We can see the same effect of the batch size on\n",
    "the variability of the batch losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-53",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_batch_loss(batch_size=32, n_batches=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_batch_loss(batch_size=128, n_batches=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_batch_loss(batch_size=8, n_batches=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046b030-edaf-4c62-a541-c8c2febe780e",
   "metadata": {},
   "source": [
    "If the batch size has an effect on the batch loss, it also affects the\n",
    "training process. Here is an example of a training process on the same\n",
    "complex loss without using mini-batches: the process here is fully\n",
    "deterministic and leads to a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-57",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"step_size\": 0.002,\n",
    "    \"epochs\": 10,\n",
    "    \"w_init\": 1.0,\n",
    "}\n",
    "\n",
    "model_1p.plot_sin_train(**train_args, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf842926-c7e8-45cd-b5e3-75e7bbccd5cf",
   "metadata": {},
   "source": [
    "If we use mini-batches during training, here with a batch size of 128,\n",
    "we can see that the training process is more erratic, and as the batches\n",
    "are shuffled between epochs, less deterministic.\n",
    "\n",
    "In the following plot, each point represents a training step, *ie* the\n",
    "loss computation and $w$ adjustment after each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_train(**train_args, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42289486-f1e3-40a4-84ba-7f511400ab73",
   "metadata": {},
   "source": [
    "With a smaller batch size, the variability between batch losses\n",
    "increases, and so the training process is even more erratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-61",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_train(**train_args, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4db7ff-ff2e-441d-96fa-ffa455b3ea7b",
   "metadata": {},
   "source": [
    "The fact that the process is more erratic and less deterministic can be\n",
    "seen as an issue, but it can also be an advantage. For example, with an\n",
    "even smaller batch size of 8, most of the training processes manage to\n",
    "“escape” the local minimum and find another, better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_train(**train_args, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2bb75-9d94-4c86-a4f7-8b2c590bf6fc",
   "metadata": {},
   "source": [
    "So, as a summary:\n",
    "\n",
    "-   A **large batch size** demands more memory, as the whole batch must\n",
    "    be loaded into the computer or GPU memory. However, since the batch\n",
    "    losses are closer to the “full” loss, the training process will be\n",
    "    smoother, more deterministic, and faster due to increased\n",
    "    computational efficiency.\n",
    "\n",
    "-   A **small batch size** requires less memory but is less\n",
    "    computationally efficient. The training process will be more erratic\n",
    "    and less deterministic. However, this can also have positive\n",
    "    consequences as it allows for a better exploration of the data\n",
    "    distribution and a greater ability to escape local minima. It will\n",
    "    be slower but in some cases can yield better results and reduce the\n",
    "    risk of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
