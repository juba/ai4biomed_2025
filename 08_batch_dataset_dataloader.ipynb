{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec4dd19-81d0-408c-a9ec-1bc904fc38ca",
   "metadata": {},
   "source": [
    "# Batches, datasets and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from adl import cooking, model_1p\n",
    "\n",
    "pn.theme_set(pn.theme_minimal() + pn.theme(plot_background=pn.element_rect(fill=\"white\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c13b38-b4bd-4a76-b90d-a220cd560728",
   "metadata": {},
   "source": [
    "Until now we only worked on very small toy datasets, but in real life\n",
    "applications of deep learning the data can be huge, either because we\n",
    "have a very large number of data points, and/or because each data point\n",
    "is itself quite big (think sequences, images or videos). In this case it\n",
    "is impossible to apply a training step (forward pass and\n",
    "backpropagation) to the whole dataset, as it will not fit in the\n",
    "computer or GPU memory.\n",
    "\n",
    "In this case the training steps will rather be applied to *mini-batches*\n",
    "of data:\n",
    "\n",
    "-   we will split our datasets into small sets of data points of a given\n",
    "    size. Each chunk is called a *mini-batch*, or a *batch*.\n",
    "-   the train step will be applied sequentially to each batch: the\n",
    "    forward pass, backpropagation and parameters adjustment will be\n",
    "    performed for each batch, one after the other.\n",
    "-   an *epoch* is reached when all the batches have been processed and\n",
    "    the entire training dataset has been seen by the network.\n",
    "\n",
    "To illustrate this process we will reuse the toy example of the\n",
    "*overfitting* notebook where people are asked to rate a cake recipe when\n",
    "the cooking time varies between 5 and 120 minutes. But this time we will\n",
    "generate a much bigger dataset of 500 000 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "time, score = cooking.generate_data(size=500_000, noise_scale=0.9)\n",
    "\n",
    "cooking.scatter_plot(time, score, size=1, alpha=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c6805c-fc15-4f59-96e5-e93759911092",
   "metadata": {},
   "source": [
    "We will define our network and our train step in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=100, out_features=100),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(in_features=100, out_features=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Center data\n",
    "        x = x - 115 / 2\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "model = RegressionNetwork()\n",
    "\n",
    "\n",
    "# Model training step\n",
    "def train_step(x, y, model, loss_fn, optimizer):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted values\n",
    "    y_pred = model(x)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagations\n",
    "    loss.backward()\n",
    "    # Parameters adjustment\n",
    "    optimizer.step()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ac7211-0746-461e-b07f-93f65226635d",
   "metadata": {},
   "source": [
    "The difference with the previous notebook is the way we will run our\n",
    "training steps.\n",
    "\n",
    "Previously, we applied it to the whole dataset at once at each epoch.\n",
    "This time we will introduce a second loop which will iterate through\n",
    "batches of data points of a given size. The train step (loss computation\n",
    "and parameters adjustment) is applied to each batch after another. At\n",
    "the end of each epoch we compute the average batch loss as the global\n",
    "epoch loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "\n",
    "epochs = 10\n",
    "# Batch size in number of data points\n",
    "batch_size = 10_000\n",
    "# Number of batches\n",
    "n_batches = len(time) // batch_size\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch in range(n_batches):\n",
    "        # For each batch, extract the corresponding x and y data\n",
    "        x_batch = time[batch * batch_size : (batch + 1) * batch_size - 1]\n",
    "        y_batch = score[batch * batch_size : (batch + 1) * batch_size - 1]\n",
    "        # Compute loss on this batch\n",
    "        batch_loss = train_step(x_batch, y_batch, model, loss_fn, optimizer)\n",
    "        # Accumulate loss between batches\n",
    "        loss += batch_loss.item()\n",
    "    # Compute average loss for this epoch\n",
    "    loss /= n_batches\n",
    "    print(f\"Epoch {epoch + 1:2} - loss: {loss:5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8e5fc-910d-4543-a0f5-d77870d2074e",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders\n",
    "\n",
    "In the previous example we performed the batches extraction manually,\n",
    "but we were in the simpler use case: the number of data points is a\n",
    "multiple of batch size, the entire data can be loaded in memory, and we\n",
    "didn’t take into account any validation dataset.\n",
    "\n",
    "In practice managing batches manually can be a bit complex and\n",
    "cumbersome, so pytorch provides two tools to make this a bit easier:\n",
    "`Datasets` and `DataLoaders`.\n",
    "\n",
    "-   A `Dataset` object describes a data source, its size and the way to\n",
    "    get an item from it. It allows to access data from a Python object,\n",
    "    a file, or any other resource accessible via Python code.\n",
    "-   A `Dataloader` allows to load data from a `Dataset` while handling\n",
    "    features like batch loading and shuffling.\n",
    "\n",
    "### Datasets\n",
    "\n",
    "To illustrate their use, we first generate sample training and\n",
    "validation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_train, score_train = cooking.generate_data(size=500_000, noise_scale=0.9)\n",
    "time_valid, score_valid = cooking.generate_data(size=18_000, noise_scale=0.9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff91eb2-841f-4814-a198-7eb0c845d536",
   "metadata": {},
   "source": [
    "The next step is to define a `Dataset` class for our data. This is a\n",
    "Python class which inherits from\n",
    "[torch.utils.data.Dataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset),\n",
    "and must implement three methods:\n",
    "\n",
    "-   `__init__()`, the class constructor\n",
    "-   `__len__()`, which must return the length of our dataset (the number\n",
    "    of data points)\n",
    "-   `__getitem__()`, which, given an integer index as argument, must\n",
    "    return a couple of `(data, label)` corresponding to this index.\n",
    "\n",
    "Here we will create a class called `RegressionDataset`:\n",
    "\n",
    "-   the constructor takes two `time` and `score` tensors as arguments\n",
    "    and store them as attributes. - the `__len__()` method returns the\n",
    "    length of these tensors.\n",
    "-   the `__getitem__()` method returns a tuple of the time and score\n",
    "    values at the given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class RegressionDataset(Dataset):\n",
    "    def __init__(self, time, score):\n",
    "        # Check if both tensors have the same length\n",
    "        if len(time) != len(score):\n",
    "            msg = \"time and score don't have the same length\"\n",
    "            raise ValueError(msg)\n",
    "        # Store time and score as attributes\n",
    "        self.time = time\n",
    "        self.score = score\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of data points\n",
    "        return len(self.score)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Returns the time and score values for the given index\n",
    "        time_index = self.time[index]\n",
    "        score_index = self.score[index]\n",
    "        return time_index, score_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eedaaaf-0e28-4e5c-81ab-18b7b62ddb20",
   "metadata": {},
   "source": [
    "Our `Dataset` is quite simple here as it just stores and retrieve values\n",
    "from tensors, but it could be more complex. For example the constructor\n",
    "could get a list of filenames containing images and their corresponding\n",
    "labels, and the `__getitem__` method would then open and read the files\n",
    "and preprocess the image data.\n",
    "\n",
    "Now that our `RegressionDataset` class is defined, we can create two\n",
    "training and validation dataset objects as `RegressionDataset`\n",
    "instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RegressionDataset(time_train, score_train)\n",
    "valid_dataset = RegressionDataset(time_valid, score_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0841a5-1c86-48a3-b316-7a4c97c594ab",
   "metadata": {},
   "source": [
    "### Dataloaders\n",
    "\n",
    "Once our `Datasets` are defined, we can create associated `Dataloaders`,\n",
    "which will handle the batches extraction and traversal. For this, we\n",
    "will create\n",
    "[torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "instances by giving it (among other possible arguments):\n",
    "\n",
    "-   a `Dataset` object\n",
    "-   the batch size\n",
    "-   a `shuffle` argument: if `True`, the data points will be reshuffled\n",
    "    randomly before each epoch. This means that batches will be\n",
    "    different from one epoch to another.\n",
    "\n",
    "We create two training and validation loaders with a batch size of 100\n",
    "000. The training loader is shuffled, to have different batches at each\n",
    "epoch. This is not useful for the validation loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 10_000\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a439808d-a8e8-4bcf-833b-aee684bd003b",
   "metadata": {},
   "source": [
    "Once created, we can iterate over a `Dataloader`. Each iteration will\n",
    "return a batch of `(time, score)` data.\n",
    "\n",
    "For example, we can iterate over our validation loader. This yields two\n",
    "batches, the first with the wanted size of 10 000, and the second one\n",
    "with a size of 8 000 (as there are only 18 000 points in our validation\n",
    "dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_loader:\n",
    "    time, score = batch\n",
    "    print(time.shape, score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9865b350-6e84-4916-8e45-0a50ffca6b96",
   "metadata": {},
   "source": [
    "If we iterate again, we will start a new epoch and get the same batches\n",
    "again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in valid_loader:\n",
    "    time, score = batch\n",
    "    print(time.shape, score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09178e-f072-400a-9058-7f871a2966a4",
   "metadata": {},
   "source": [
    "Now we can rewrite our training code by using our data loaders. Inside\n",
    "each epoch, we will first iterate through our `train_loader` object and\n",
    "run a training step on the yielded batch. Then, once all training\n",
    "batches have been processed, we will iterate through our `valid_loader`,\n",
    "this time to compute the validation loss for this epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RegressionNetwork()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.005)  # type: ignore\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "\n",
    "    # Iterate through training batches\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # Set model in train mode\n",
    "        model.train()\n",
    "        # Run a training step and accumulate loss value\n",
    "        batch_loss = train_step(x_batch, y_batch, model, loss_fn, optimizer)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "    # Compute average training loss for this epoch\n",
    "    loss /= len(train_loader)\n",
    "\n",
    "    # Iterate through validation batches\n",
    "    valid_loss = 0\n",
    "    for x_valid_batch, y_valid_batch in valid_loader:\n",
    "        # Set model in evaluation (inference) mode\n",
    "        model.eval()\n",
    "        # Compute and accumulate the batch loss\n",
    "        y_valid_pred = model(x_valid_batch)\n",
    "        valid_batch_loss = loss_fn(y_valid_pred, y_valid_batch)\n",
    "        valid_loss += valid_batch_loss\n",
    "\n",
    "    # Compute the average validation loss for this epoch\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    print(f\"{epoch + 1:5}. loss: {loss:5.3f}, valid_loss: {valid_loss:5.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af19a1b9-c55c-485f-be3f-f1fab8529ff4",
   "metadata": {},
   "source": [
    "The training is working well, but if it is much slower than previously,\n",
    "even if we take into account that we now also compute the validation\n",
    "loss. However, the slowdown would be smaller if the cost of the\n",
    "`__getitem__()` operation was higher (if, for example, we were reading a\n",
    "file). And we have some nice bonus features, such as managing\n",
    "automatically the size of the last batch, or the shuffling of training\n",
    "data between each epoch.\n",
    "\n",
    "## Effect of batch size on training process\n",
    "\n",
    "Besides allowing to train a model from bigger datasets, the use of\n",
    "mini-batches also has an effect on the training process itself. This is\n",
    "due to the fact that when using batches, the loss function will be\n",
    "slightly different at each train step (and thus the gradients and the\n",
    "parameters adjustments will also be slightly different).\n",
    "\n",
    "To illustrate this point, we reuse a previous example of linear\n",
    "regression with only one parameter $w$ (the slope of the line). We first\n",
    "generate a random dataset of 500 points of `x` and `y` values where `y`\n",
    "is equal to `x * 2` plus some noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 500\n",
    "np.random.seed(1337)\n",
    "# Generate x and y values\n",
    "x = np.random.uniform(low=0, high=10, size=n_points)\n",
    "y = x * 2 + np.random.normal(loc=0, scale=3, size=n_points)\n",
    "\n",
    "# Convert to tensors\n",
    "xt = torch.tensor(x).view(-1, 1)\n",
    "yt = torch.tensor(y).view(-1, 1)\n",
    "\n",
    "# Plot the dataset\n",
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=x, y=y))\n",
    "    + pn.geom_abline(slope=2, intercept=0, color=\"red\")\n",
    "    + pn.geom_point(color=\"royalblue\", size=2, alpha=0.5)\n",
    "    + pn.coord_cartesian(xlim=(0, 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52439fd1-87f5-4c89-ba72-3317173a709b",
   "metadata": {},
   "source": [
    "We can plot the loss function for our whole dataset, *ie*, the loss\n",
    "value computed on our data for different values of our parameter $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_loss(xt, yt, wmin=-4, wmax=8, gradient=False, ylim=(0, 2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88dadc7-927c-421c-b315-d05035d8de72",
   "metadata": {},
   "source": [
    "We can see that if $w = 0$ (*ie* with an horizontal regression line),\n",
    "the loss value on our dataset is about 200. As expected, the minimum\n",
    "value of loss is reached for a value of $w$ approximately equal to 2.\n",
    "\n",
    "But what happens if we compute this loss function not over the whole\n",
    "dataset, but only over a subset of it, for example of 32 data points?\n",
    "\n",
    "We can visualize this by randomly sampling 32 points from our dataset,\n",
    "plotting the associated loss function (in grey on the next plot) and\n",
    "compare it with the loss function of our whole data (the dashed red\n",
    "line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=32, n_batches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dedd62-fdb2-455b-b73b-ee69aa4dc5aa",
   "metadata": {},
   "source": [
    "We can see that the loss of our batch is not identical to the “full”\n",
    "loss. It has about the same shape but its values are not the same.\n",
    "\n",
    "We can guess that each batch of 32 points will generate a different loss\n",
    "function. We can visualize this variability by generating many batches\n",
    "and plotting their losses on the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=32, n_batches=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d4d0dc-8c26-46a3-ae60-3365657eaad3",
   "metadata": {},
   "source": [
    "What happens if we decrease the batch size?\n",
    "\n",
    "If we create batches of 16 data points, we can see that the variability\n",
    "around the “full” loss is higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=16, n_batches=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8810a91-dea5-4370-9dbd-088a4f4bfab6",
   "metadata": {},
   "source": [
    "As an extreme example, with a batch size of 1, the loss function is\n",
    "calculated for only one data point. The variability of the loss is then\n",
    "maximal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=1, n_batches=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef97f56c-6c9d-4c01-8f7e-ef5aae0d8953",
   "metadata": {},
   "source": [
    "On the contrary, with a larger batch size of 256, the loss functions\n",
    "will more closely approximate the “full” loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_batch_loss(xt, yt, batch_size=256, n_batches=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb218c29-7daf-4e2a-be2b-59b48fcbc6ea",
   "metadata": {},
   "source": [
    "Here is another example with a more complex loss function, still with a\n",
    "unique parameter $w$. We can see the same effect of the batch size on\n",
    "the variability of the batch losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_batch_loss(batch_size=128, n_batches=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_batch_loss(batch_size=8, n_batches=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ab737c-a27a-4e9c-a3b1-21f24e0e71d9",
   "metadata": {},
   "source": [
    "If the batch size has an effect on the batch loss, it also affects the\n",
    "training process. Here is an example of a training process on the same\n",
    "complex loss without using mini-batches: the process here is fully\n",
    "deterministic and leads to a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"step_size\": 0.002,\n",
    "    \"epochs\": 10,\n",
    "    \"w_init\": 1.0,\n",
    "}\n",
    "\n",
    "model_1p.plot_sin_train(**train_args, batch_size=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f31a3-045a-4022-aba9-3becdbb6b4da",
   "metadata": {},
   "source": [
    "If we use mini-batches during training, here with a batch size of 128,\n",
    "we can see that the training process is more erratic, and as the batches\n",
    "are shuffled between epochs, less deterministic.\n",
    "\n",
    "In the following plot, each point represents a training step, *ie* the\n",
    "loss computation and $w$ adjustment after each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_train(**train_args, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9d0f9-a400-4305-93f8-edcc822a00c8",
   "metadata": {},
   "source": [
    "With a smaller batch size, the variability between batch losses\n",
    "increases, and so the training process is even more erratic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_train(**train_args, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc62e18-2b89-475e-a1e2-0a170b474dc2",
   "metadata": {},
   "source": [
    "The fact that the process is more erratic and less deterministic can be\n",
    "seen as an issue, but it can also be an advantage. For example, with an\n",
    "even smaller batch size of 8, most of the training processes manage to\n",
    "“escape” the local minimum and find another, better one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-48",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_sin_train(**train_args, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5ab46-6e57-4fee-b3bb-3fa7316c23f3",
   "metadata": {},
   "source": [
    "So, as a summary:\n",
    "\n",
    "-   A **large batch size** demands more memory, as the whole batch must\n",
    "    be loaded into the computer or GPU memory. However, since the batch\n",
    "    losses are closer to the “full” loss, the training process will be\n",
    "    smoother, more deterministic, and faster due to increased\n",
    "    computational efficiency.\n",
    "\n",
    "-   A **small batch size** requires less memory but is less\n",
    "    computationally efficient. The training process will be more erratic\n",
    "    and less deterministic. However, this can also have positive\n",
    "    consequences as it allows for a better exploration of the data\n",
    "    distribution and a greater ability to escape local minima. It will\n",
    "    be slower but in some cases can yield better results and reduce the\n",
    "    risk of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
