{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2e44db5-d9b8-41af-9c51-57131787fa83",
   "metadata": {},
   "source": [
    "# Learning rate\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "```shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotnine as pn\n",
    "import torch\n",
    "\n",
    "from adl.sklearn import skl_regression\n",
    "\n",
    "pn.theme_set(\n",
    "    pn.theme_minimal() + pn.theme(plot_background=pn.element_rect(fill=\"white\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927149f8-f052-4c5f-b47f-ae91e84c6a2b",
   "metadata": {},
   "source": [
    "In this notebook we will take a look at the effect of the step size, or\n",
    "learning rate, on the training process. For this we will use a very\n",
    "simple linear regression example with only one parameter (the slope of\n",
    "the regression line).\n",
    "\n",
    "## Data\n",
    "\n",
    "We start with some random input and target data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "x_values = [2.1, 3.4, 1.8, 5.9, 8.3, 9.1, 2.4, 5.6, 7.8]\n",
    "# Target data\n",
    "y_values = [4.0, 6.2, 3.0, 13.2, 17.8, 17.9, 5.5, 11.7, 14.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11755564-a958-4f6b-9e81-d35d0a45c77b",
   "metadata": {},
   "source": [
    "We want to predict $y$ values from $x$ with a model of the form\n",
    "$y = x \\times w$, _ie_ a linear regression without intercept. Our model\n",
    "has only one parameter, the **weight** $w$, which represents the slope\n",
    "of the rgression line. Our goal is to estimate the optimal value of $w$\n",
    "parameter from the data: this optimal value will be the one which\n",
    "minimizes the mean squared error, _ie_ the mean of squared distances\n",
    "between predictions and targets.\n",
    "\n",
    "## Regression with scikit-learn\n",
    "\n",
    "As a reference, we can compute the optimal $w$ value by doing a simple\n",
    "linear regression without intercept, for example with `scikit-learn`. We\n",
    "will use a predefined custom function to do this, and display both the\n",
    "best $w$ value and the associated minimum loss value (here the loss is\n",
    "the mean squared error).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = skl_regression(x_values, y_values, fit_intercept=False)\n",
    "print(f\"slope: {reg['slope']:.2f}, mse: {reg['mse']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe28dd34-6e33-47a0-8053-7a11d73465f7",
   "metadata": {},
   "source": [
    "We can plot both our data points and the estimated regression line.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pn.ggplot(mapping=pn.aes(x=x_values, y=y_values))\n",
    "    + pn.geom_abline(slope=reg[\"slope\"], intercept=0, color=\"orchid\")\n",
    "    + pn.geom_point(fill=\"yellowgreen\", color=\"white\", size=4)\n",
    "    + pn.coord_cartesian(xlim=(0, 10), ylim=(0, 20))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b5a19-2e87-48a5-b696-56f0fcc0a8e0",
   "metadata": {},
   "source": [
    "## Regression with pytorch\n",
    "\n",
    "We have seen in the previous notebook that we can do the same\n",
    "computation (finding the value of $w$ that minimizes the mean squared\n",
    "error) with pytorch. In this case, instead of computing $w$ directly we\n",
    "will approximate its value with a gradient descent.\n",
    "\n",
    "**Exercise 1**\n",
    "\n",
    "Using pytorch, write and run the code implementing the training process\n",
    "to find the value of $w$ which minimizes the mean square error loss\n",
    "between true and predicted values.\n",
    "\n",
    "Run a training process with a step size (or learning rate) of 0.001 for\n",
    "10 epochs and print the `w` value at each epoch.\n",
    "\n",
    "## Effect of step size (learning rate)\n",
    "\n",
    "For convenience, for now on we will use a predefined function that will\n",
    "run the training process while keeping track of the different loss,\n",
    "gradient and weight values at each training step in order to easily\n",
    "compare the results for different step size values.\n",
    "\n",
    "Here are the results with a step size of 0.001. The `new_w` column\n",
    "should have the same values as the output of the code you wrote for the\n",
    "exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from adl import model_1p\n",
    "\n",
    "# Convert x and y data to tensors\n",
    "x = torch.tensor(x_values)\n",
    "y = torch.tensor(y_values)\n",
    "\n",
    "model_1p.train(x, y, step_size=0.001, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abe1c9f-c5d4-4770-aa8a-9fb504204107",
   "metadata": {},
   "source": [
    "We can see that the weight $w$ evolves towards the optimum value while\n",
    "the loss goes down, but the training is quite slow and the optimum is\n",
    "not reached after 10 iterations.\n",
    "\n",
    "With a larger step size of 0.01, the optimal $w$ value and the\n",
    "associated minimal loss are reached after only a few training steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.train(x, y, step_size=0.01, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98209157-8efb-4f75-9544-2e95b57d8c83",
   "metadata": {},
   "source": [
    "With an even larger step size of 0.1, the result is completely\n",
    "different. The loss, instead of going down, is increasing at each step.\n",
    "Accordingly, the weight value goes farther and farther from the optimal\n",
    "one.\n",
    "\n",
    "Our model is _diverging_, and adding more training steps would only make\n",
    "$w$ go farther from its optimal value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.train(x, y, step_size=0.1, epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5666648-8396-4953-9f9c-d6918c73871f",
   "metadata": {},
   "source": [
    "## Graphical representations\n",
    "\n",
    "The following plot shows the value of the loss function for $w$ values\n",
    "ranging from -1 to 4. We can see that the loss is minimal when $w$ is\n",
    "around 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_loss(x, y, wmin=-1, wmax=4, gradient=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8134e625-7307-4a97-9756-0fc906528fe2",
   "metadata": {},
   "source": [
    "The next plot is an attempt at visualizing the _gradient_ value of the\n",
    "loss at different $w$ values. The direction of the red arrow at a given\n",
    "point depends on the sign of the gradient at this point, and it\n",
    "indicates the “direction” we should go if we want the loss to go up. So,\n",
    "at a given point of the curve, if we want to minimize the loss value we\n",
    "have to modifiy $w$ in the direction _opposite_ to the one of the arrow.\n",
    "\n",
    "The length of the arrow is proportional to the gradient absolute value.\n",
    "It represents the intensity of the modification of the loss value in the\n",
    "gradient direction: if the arrow is long, then moving $w$ a bit in this\n",
    "direction will lead to a higher gradient increase. If it is short, it\n",
    "will lead to a smaller increase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_loss(x, y, wmin=-1, wmax=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eefc06-7c69-48c7-8f9a-463633fe6bd8",
   "metadata": {},
   "source": [
    "We can also try to visualize the _training process_.\n",
    "\n",
    "The following plot shows the values of $w$ at each step of a training\n",
    "process starting from $w=0$ and running for 30 epochs with a step size\n",
    "of 0.001. We see that at each step $w$ follows the loss function curve\n",
    "to go towards its minimum, even if it is not reached after 30 epochs. We\n",
    "can also see that the “move” of $w$ value is smaller and smaller at each\n",
    "epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_train(x, y, step_size=0.001, epochs=30, wmin=-2, wmax=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace5dc4-9449-4bb3-8000-4381f7de2171",
   "metadata": {},
   "source": [
    "If we increase the step size to 0.01, we see that the training process\n",
    "is much faster, and $w$ goes towards its optimum by moving more rapidly,\n",
    "and the minimum is reached after a few epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_train(x, y, step_size=0.01, epochs=10, wmin=-2, wmax=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f118c-62c3-4233-acc1-42a5e1190ff3",
   "metadata": {},
   "source": [
    "With a learning rate of 0.025, the training process is working but a bit\n",
    "differently: $w$ moves even “faster”, but by doing so it “overshoots”\n",
    "and goes beyond the minimum. but the process is nevertheless converging,\n",
    "because by going from one side of the optimum to the other, it manages\n",
    "to get closer eat each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_train(x, y, step_size=0.025, epochs=8, wmin=-2, wmax=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e90f44-18d1-4bd7-b3a8-20c70c871c74",
   "metadata": {},
   "source": [
    "If we increase the learning rate a bit more, we can get to a situation\n",
    "where the training process seems almost stalled: the $w$ value goes from\n",
    "one side of the minimal value to the other, but barely progressing\n",
    "towards it even after 20 epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_train(x, y, step_size=0.02963, epochs=20, wmin=-2, wmax=4.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de22379d-d52a-4c19-8be0-1c9a38058e76",
   "metadata": {},
   "source": [
    "Finally, with a learning rate even higher, we reach a point when $w$\n",
    "“moves too much”, and the process becomes diverging: at each step the\n",
    "loss becomes higher and $w$ goes farther instead of closer from its\n",
    "optimum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1p.plot_train(x, y, step_size=0.031, epochs=8, wmin=-2, wmax=4.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
