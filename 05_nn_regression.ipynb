{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe19eb3f-8d83-4e09-abe5-a3302af32dcb",
   "metadata": {},
   "source": [
    "# Regression with a (very very simple) pytorch neural network\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "``` shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from torchinfo import summary\n",
    "\n",
    "from adl.sklearn import skl_regression\n",
    "\n",
    "pn.theme_set(pn.theme_minimal())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca20fc-2467-41a7-bcd5-1cced3880419",
   "metadata": {},
   "source": [
    "In the previous notebooks, we used gradient descent to solve simple\n",
    "linear regression problems. In this notebook we introduce a way to do\n",
    "the same thing but using a (very simple) neural network defined with\n",
    "pytorch syntax.\n",
    "\n",
    "We will reuse our fake data about temperature and ice cream sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c96b2ff-07ae-4206-bc61-65c0edce9221",
   "metadata": {},
   "source": [
    "As seen previously, we scale the `temperature` values in order to\n",
    "improve the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_s = preprocessing.scale(temperature, with_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294021b6-69a8-4847-b542-b16bda0ce766",
   "metadata": {},
   "source": [
    "We then compute the “real” optimal slope, optimal intercept and minimal\n",
    "loss with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = skl_regression(temperature_s, icecream)\n",
    "print(f\"slope: {reg['slope']:.2f}, intercept: {reg['intercept']:.2f}, mse: {reg['mse']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae10e62-219d-453f-8346-8fda896b75c8",
   "metadata": {},
   "source": [
    "Finally, we transform our input and target values into tensors. One\n",
    "difference here is that we have to reshape our data: pytorch requires to\n",
    "have each observation and target in its own array, so for example the\n",
    "temperatures `[100.5, 110.2, 133.5]` must be converted to\n",
    "`[[100.5], [110.2], [133.5]]`. In other words, our input and target data\n",
    "are now arrays with one column instead of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(temperature_s).float().view(-1, 1)\n",
    "y = torch.tensor(icecream).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b6a49b-4dc6-4ad6-a3d7-74da821778ff",
   "metadata": {},
   "source": [
    "## Regression with pytorch and a single neuron neural network\n",
    "\n",
    "In the previous notebooks, we created our model by just creating a\n",
    "simple `forward` function, like this:\n",
    "\n",
    "``` python\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "```\n",
    "\n",
    "This approach is adequate for a simple model like we have now, but for\n",
    "more complex models such as neural networks, we will need to use\n",
    "pytorch’s built-in functions to define them.\n",
    "\n",
    "In fact, a simple linear regression with a single explanatory variable\n",
    "can be considered as a neural “network” with just one neuron. So we will\n",
    "try to rewrite our simple model using pytorch’s notation.\n",
    "\n",
    "One way to define our “network” is to use the *Module* notation,\n",
    "provided by `torch.nn.Module`. This notation forces to create a new\n",
    "Python class, which inherits from `nn.Module`, and then to create at\n",
    "least an `__init__()` method (called when the model is created) and a\n",
    "`forward()` method, which takes input data as argument, applies our\n",
    "model and returns the predicted values.\n",
    "\n",
    "To create our simple linear regression model, we will use `nn.Linear`,\n",
    "which allows to define linear layers of arbitrary size. Here our layer\n",
    "will have a single neuron which will take a single number as input (a\n",
    "temperature value) and will output a single (a predicted ice cream sale\n",
    "volume). In pytorch notation, this means that our layer will have\n",
    "`in_features` of size 1, and `out_features` of size 1.\n",
    "\n",
    "Here is the code of a `LinearNetwork` class which implements this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LinearNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear regression model with only one input variable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Call the parent constructor (mandatory)\n",
    "        super().__init__()\n",
    "        # Create a \"linear\" attribute which will contain a linear layer with input and\n",
    "        # output of size 1\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Method which implements the model forward pass, ie which takes input data as\n",
    "        argument, applies the model to it and returns the result.\n",
    "        \"\"\"\n",
    "        # Apply our linear layer to input data\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ae80c-2e65-446c-a262-cc38928eacf0",
   "metadata": {},
   "source": [
    "Once our model class has been created, we can use it to create a new\n",
    "model object (or model instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354627e-5b7c-4e62-bf50-dd2a2ed12342",
   "metadata": {},
   "source": [
    "It is important to distinguish between:\n",
    "\n",
    "-   a model class, like `LinearNetwork`, which is a Python class\n",
    "    describing a model architecture\n",
    "-   a model object or model instance, like `model`, which is a concrete\n",
    "    model created using the `LinearNetwork` architecture\n",
    "\n",
    "We can use the `summary` function of the `torchinfo` package to display\n",
    "a description of our `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebd95ac-322c-4bee-bb85-3bf5c0b6464f",
   "metadata": {},
   "source": [
    "We can see that `model` has one layer and two parameters: the weight and\n",
    "the bias of our single “neuron”. We can see that pytorch take care of\n",
    "creating these parameters, we don’t have to manually create `w` and `b`\n",
    "tensors anymore.\n",
    "\n",
    "We can pass input data directly to our `model` object. In this case, it\n",
    "will call the `model.forward()` which applies the model to the input\n",
    "data to compute prediction. We can see that both are equivalent (the\n",
    "predictions here are random because `model` parameters have been\n",
    "initialized randomly during `model` creation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2172f141-1dc8-43b5-ae9e-64395e27f635",
   "metadata": {},
   "source": [
    "We can now build our training process. We will use `MSELoss()` as loss\n",
    "function, and an `SGD` optimizer with a learning rate of 0.1. However,\n",
    "instead of explicitly passing a list of parameters like`[w, b]` as first\n",
    "optimizer argument, we will use `model.parameters()` which will\n",
    "automatically provide all the parameters of our `model` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1a6af5-3cae-4649-ad61-0b33d99d93bd",
   "metadata": {},
   "source": [
    "Finally, we define and run our training loop for a certain number of\n",
    "epochs:\n",
    "\n",
    "-   we start by resetting our gradient with `optimizer.zero_grad()`\n",
    "-   we compute the predicted values by applying our `model` object to\n",
    "    the input data (forward pass)\n",
    "-   we compute the loss value\n",
    "-   we compute the loss gradient for each parameter (backpropagation)\n",
    "-   finally we adjust our parameters by calling `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode - important for batch normalization and dropout\n",
    "    # layers. Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted values\n",
    "    y_pred = model(x)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Parameters adjustment\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print results for this epoch. We can get the weight and bias values by accessing the\n",
    "    # \"weight\" and \"bias\" attributes of the model.linear layer\n",
    "    print(\n",
    "        f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model.linear.weight.item():5.2f},\"\n",
    "        f\" bias: {model.linear.bias.item():6.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4493156-bec0-4cb0-ac07-4a7a8626acd7",
   "metadata": {},
   "source": [
    "We can see that our training process seems to converge towards the\n",
    "“true” values computed above.\n",
    "\n",
    "## Regression with two explanatory variables\n",
    "\n",
    "If we want to do a linear regression with two explanatory variables, our\n",
    "input data `X` will now be an array with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "humidity = [50.1, 34.8, 51.3, 64.1, 47.8, 53.4, 58.0, 71.5, 32.0, 43.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]\n",
    "\n",
    "\n",
    "X = np.array([temperature, humidity]).transpose()\n",
    "X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cd0c82-ac9f-4bf2-8c56-25d243f6a7f9",
   "metadata": {},
   "source": [
    "We can scale our input data `X` and convert `X` and `y` to tensors as\n",
    "usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocessing.scale(X)\n",
    "X = torch.tensor(X).float()\n",
    "\n",
    "y = torch.tensor(icecream).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02f8021-6de9-4807-bfa3-b9d0fea26bd7",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "-   create a new model class `LinearNetwork2` which will have the same\n",
    "    architecture of `LinearNetwork` but will accept input data of\n",
    "    dimension 2.\n",
    "-   create a new `model2` object from the `LinearNetwork2` class\n",
    "-   display a summary description of `model2`\n",
    "-   run a training loop of `model2` for 20 epochs with an `MSELoss` loss\n",
    "    and an `SGD` optimizer with a 0.1 learning rate\n",
    "\n",
    "*Hint :* if you want to display the weights and bias values at each\n",
    "epoch, you can use `model2.linear.weight.data` and\n",
    "`model2.linear.bias.item()`.\n",
    "\n",
    "### Generalization to any number of explanatory variables\n",
    "\n",
    "**Exercise 2**\n",
    "\n",
    "We created two different classes above: one for a linear regression\n",
    "model with only one explanatory variable, and one for two explanatory\n",
    "variables. Now we will try to create a more generic model class that can\n",
    "return models accepting any number of explanatory variables.\n",
    "\n",
    "-   Create a new `GeneralLinearNetwork` class by starting from the\n",
    "    `LinearNetwork` class seen above\n",
    "-   Modify the `__init__()` method so that it accepts a new argument\n",
    "    called `n_variables`\n",
    "-   Modify the `self.linear` creation so that it takes into account the\n",
    "    value passed as `n_variables` argument\n",
    "\n",
    "Once the class has been created:\n",
    "\n",
    "-   instanciate a model object called `model1` which accepts input data\n",
    "    with one column and apply it to the `x` input data\n",
    "-   instanciate a model object called `model2` which accepts input data\n",
    "    with two columns and apply it to the `X` input data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
