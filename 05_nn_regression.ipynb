{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7e0227-f28c-476d-a5ac-63c9bdc50d19",
   "metadata": {},
   "source": [
    "# Regression with a (very very simple) pytorch neural network\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "```shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from torchinfo import summary\n",
    "\n",
    "from adl.sklearn import skl_regression\n",
    "\n",
    "pn.theme_set(pn.theme_minimal())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1197e499-7ffc-4f53-a552-a9ec41534bb6",
   "metadata": {},
   "source": [
    "In the previous notebooks, we used gradient descent to solve simple\n",
    "linear regression problems. In this notebook we introduce a way to do\n",
    "the same thing but using a (very simple) neural network defined with\n",
    "pytorch syntax.\n",
    "\n",
    "We will reuse our fake data about temperature and ice cream sales seen\n",
    "previously.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37521cd-d0fd-4868-9ef0-e01ef2b3c906",
   "metadata": {},
   "source": [
    "As seen previously, we scale the `temperature` values in order to\n",
    "improve the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_s = preprocessing.scale(temperature, with_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059518d-5e09-4686-9372-7497fb6a1745",
   "metadata": {},
   "source": [
    "We then compute the “real” optimal slope and intercept values and\n",
    "minimal loss with `scikit-learn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = skl_regression(temperature_s, icecream)\n",
    "print(\n",
    "    f\"slope: {reg['slope']:.2f}, intercept: {reg['intercept']:.2f}, mse: {reg['mse']:.4f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8185d-1059-4023-b8df-8986cf84ded4",
   "metadata": {},
   "source": [
    "Finally, we transform our input and target values to tensors. One\n",
    "difference here is that we have to reshape our data: pytorch requires to\n",
    "have each observation and target in its own array, so for example the\n",
    "temperatures `[100.5, 110.2, 133.5]` must be converted to\n",
    "`[[100.5], [110.2], [133.5]]`. In other words, our input and target data\n",
    "are now arrays with one column instead of vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(temperature_s).float().view(-1, 1)\n",
    "y = torch.tensor(icecream).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974dcda-b9f0-44fc-b5a4-7b4c2df22741",
   "metadata": {},
   "source": [
    "## Regression with pytorch and a single neuron neural network\n",
    "\n",
    "In the previous notebooks, we created our model by just creating a\n",
    "simple `forward` function, like this:\n",
    "\n",
    "```python\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "```\n",
    "\n",
    "This is suitable for a very simple model like this one, but for more\n",
    "complex models like a neural network, we will have to use the pytorch\n",
    "functions to define it.\n",
    "\n",
    "In fact, a simple linear regression with only one explanatory variable\n",
    "can be seen as a neural “network” with only a single neuron. So we will\n",
    "try to convert our simple model to use pytorch notation.\n",
    "\n",
    "One way to define our “network” is to use the _Module_ notation,\n",
    "provided by `torch.nn.Module`. This notation forces to create a new\n",
    "Python class, which inherits from `nn.Module`, and then to create at\n",
    "least an `__init__()` method (called when the model is created) and a\n",
    "`forward()` method, which takes input data as argument, applies our\n",
    "model and returns the predicted values.\n",
    "\n",
    "To create our simple linear regression model, we will use `nn.Linear`,\n",
    "which allows to define linear layers of arbitrary size. Here our layer\n",
    "will have a single neuron which will take a single number as input (a\n",
    "temperature value) and will output a single number as output (a\n",
    "predicted ice cream sale volume). In pytorch notation, this means that\n",
    "our layer will have `in_features` of size 1, and `out_features` of size\n",
    "\n",
    "1.\n",
    "\n",
    "Here is the code of a `LinearRegressionNetwork` class which implements\n",
    "this model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LinearRegressionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear regression model with only one input variable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Call the parent constructor (mandatory)\n",
    "        super().__init__()\n",
    "        # Create a \"linear\" attribute which will contain a linear layer with input and\n",
    "        # output of size 1\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Method which implements the model forward pass, ie which takes input data as\n",
    "        argument, applies the model to it and returns the result.\n",
    "        \"\"\"\n",
    "        # Apply our linear layer to input data\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86d6c8-08ab-4edf-9f85-fee0f6dc8532",
   "metadata": {},
   "source": [
    "Once our class has been created, we can use it to create a model object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d0b39-9ec1-4ac1-af20-64dc71c1a235",
   "metadata": {},
   "source": [
    "By displaying a summary description of our model we can see that it has\n",
    "two parameters: the weight and the bias of our single “neuron”. We can\n",
    "see that pytorch take cares of creating these parameters, we don’t have\n",
    "to manually create `w` and `b` tensors anymore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd420aa-789d-43ab-ac2c-2b2551f79804",
   "metadata": {},
   "source": [
    "Once our model class has been created and our model object ha been\n",
    "instanciated, we can build our training process. As seen previously, we\n",
    "will use `MSELoss()` as loss function, and an `SGD` optimizer with a\n",
    "learning rate of 0.1. However, instead of explicitly passing a list of\n",
    "parameters as first optimizer argument, we will use `model.parameters()`\n",
    "which will automatically provide all the parameters of our `model`\n",
    "object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa55aca-351f-4c3e-83e1-e5c5fe845be8",
   "metadata": {},
   "source": [
    "Finally, we define and run our training loop for a certain number of\n",
    "epochs:\n",
    "\n",
    "-   we start by resetting our gradient with `optimizer.zero_grad()`\n",
    "-   we compute the predicted values by applying our `model` object to\n",
    "    the input data (forward pass)\n",
    "-   we compute the loss value\n",
    "-   we compute the loss gradient for each parameter (backpropagation)\n",
    "-   finally we adjust our model parameters by calling `optimizer.step()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode - important for batch normalization and dropout\n",
    "    # layers. Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted values\n",
    "    y_pred = model(x)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Parameters adjustment\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print results for this epoch. We can get the weight and bias values by accessing the\n",
    "    # \"weight\" and \"bias\" attributes of the model.linear layer\n",
    "    print(\n",
    "        f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model.linear.weight.item():5.2f},\"\n",
    "        f\" bias: {model.linear.bias.item():6.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a00d21-a0a7-4b68-9c09-c73480723c5f",
   "metadata": {},
   "source": [
    "We can see that our training process seems to converge towards the\n",
    "“true” values computed above.\n",
    "\n",
    "## Regression with two explanatory variables\n",
    "\n",
    "If we want to do a linear regression with two explanatory variables, our\n",
    "input data `X` will now be a tensor with two columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "humidity = [50.1, 34.8, 51.3, 64.1, 47.8, 53.4, 58.0, 71.5, 32.0, 43.5]\n",
    "X = preprocessing.scale(np.array([temperature, humidity]).transpose())\n",
    "X = torch.tensor(X).float()\n",
    "\n",
    "# Target values\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]\n",
    "y = torch.tensor(icecream).float().view(-1, 1)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9fa9c7-0938-470d-9b5a-84c662654335",
   "metadata": {},
   "source": [
    "As previously, we will create a new class representing our model, with\n",
    "an `__init__()` and a `forward()` methods. The class is almost\n",
    "identical, except that our `Linear` layer will now have 2 inputs instead\n",
    "of 1 (but still 1 output only).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model = LinearRegressionNetwork2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345ddbb1-e3b4-4ad3-ae1f-26d24a3665ee",
   "metadata": {},
   "source": [
    "We can see that our model now has 3 parameters: two weights (one for\n",
    "each input) and one bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589477ba-000c-4e51-88a3-36caf2e521ec",
   "metadata": {},
   "source": [
    "The training loop is the same as the previous one. The only difference\n",
    "is that `model.linear.weight` now contains two values instead of one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # type: ignore\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode - important for batch normalization and dropout\n",
    "    # layers. Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted values\n",
    "    y_pred = model(X)\n",
    "    # Copute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Parameters adjustment\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\n",
    "        f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model.linear.weight.data},\"\n",
    "        f\" bias: {model.linear.bias.item():6.1f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef957b6-00a4-4551-8091-a906eddd964e",
   "metadata": {},
   "source": [
    "### Generalization to any number of explanatory variables\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "We created two different classes above: one for a linear regression\n",
    "model with only one explanatory variable, and one for two explanatory\n",
    "variables. Now we will try to create a more generic model class that can\n",
    "return models accepting any number of explanatory variables.\n",
    "\n",
    "-   Create a new `GeneralLinearRegressionNetwork` class by starting from\n",
    "    the `LinearRegressionNetwork` class seen above\n",
    "-   Modify the `__init__()` method so that it accepts a new argument\n",
    "    called `n_variables`\n",
    "-   Modify the `self.linear` creation so that it takes into account the\n",
    "    value passed as `n_variables` argument\n",
    "\n",
    "Once the class has been created:\n",
    "\n",
    "-   instanciate a model object called `model1` which accepts input data\n",
    "    with one column and apply it to the `x` input data\n",
    "-   instanciate a model object called `model2` which accepts input data\n",
    "    with two columns and apply it to the `X` input data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
