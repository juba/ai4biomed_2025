{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e386d5b5-0fd6-487b-a3ca-0bc6dcdab30c",
   "metadata": {},
   "source": [
    "# Regression with a (very very simple) pytorch neural network\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "``` shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotnine as pn\n",
    "import torch\n",
    "from sklearn import preprocessing\n",
    "from torchinfo import summary\n",
    "\n",
    "from adl.sklearn import skl_regression\n",
    "\n",
    "pn.theme_set(pn.theme_minimal())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fe57b1-3192-4a4f-9e52-35c13eaf0118",
   "metadata": {},
   "source": [
    "In the previous notebooks, we used gradient descent to solve simple\n",
    "linear regression problems. In this notebook we introduce a way to do\n",
    "the same thing but using a (very simple) neural network defined with\n",
    "pytorch syntax.\n",
    "\n",
    "We will reuse our fake data about temperature and ice cream sales seen\n",
    "previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28bd92-4e2a-464d-a111-79d1bc38dfef",
   "metadata": {},
   "source": [
    "As seen previously, we scale the `temperature` values in order to\n",
    "improve the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_s = preprocessing.scale(temperature, with_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f763500f-fdb0-4d90-a15a-9e54f7bc7a35",
   "metadata": {},
   "source": [
    "We then compute the “real” optimal slope and intercept values and\n",
    "minimal loss with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = skl_regression(temperature_s, icecream)\n",
    "print(f\"slope: {reg['slope']:.2f}, intercept: {reg['intercept']:.2f}, mse: {reg['mse']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a09df6-e71b-41b1-a0e3-73efe366660f",
   "metadata": {},
   "source": [
    "Finally, we transform our input and target values to tensors. One\n",
    "difference here is that we have to reshape our data: pytorch requires to\n",
    "have each observation and target in its own array, so for example the\n",
    "temperatures `[100.5, 110.2, 133.5]` must be converted to\n",
    "`[[100.5], [110.2], [133.5]]`. In other words, our input and target data\n",
    "are now arrays with one column instead of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(temperature_s).float().view(-1, 1)\n",
    "y = torch.tensor(icecream).float().view(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8727a151-4aeb-46f7-a54d-9e303aa59d4f",
   "metadata": {},
   "source": [
    "## Regression with pytorch and a single neuron neural network\n",
    "\n",
    "In the previous notebooks, we created our model by just creating a\n",
    "simple `forward` function, like this:\n",
    "\n",
    "``` python\n",
    "def forward(x):\n",
    "    return w * x + b\n",
    "```\n",
    "\n",
    "This is suitable for a very simple model like this one, but for more\n",
    "complex models like a neural network, we will have to use the pytorch\n",
    "functions to define it.\n",
    "\n",
    "In fact, a simple linear regression with only one explanatory variable\n",
    "can be seen as a neural “network” with only a single neuron. So we will\n",
    "try to convert our simple model to use pytorch notation.\n",
    "\n",
    "One way to define our “network” is to use the *Module* notation,\n",
    "provided by `torch.nn.Module`. This notation forces to create a new\n",
    "Python class, which inherits from `nn.Module`, and then to create at\n",
    "least an `__init__()` method (called when the model is created) and a\n",
    "`forward()` method, which takes input data as argument, applies our\n",
    "model and returns the predicted values.\n",
    "\n",
    "To create our simple linear regression model, we will use `nn.Linear`,\n",
    "which allows to define linear layers of arbitrary size. Here our layer\n",
    "will have a single neuron which will take a single number as input (a\n",
    "temperature value) and will output a single number as output (a\n",
    "predicted ice cream sale volume). In pytorch notation, this means that\n",
    "our layer will have `in_features` of size 1, and `out_features` of size\n",
    "1.\n",
    "\n",
    "Here is the code of a `LinearRegressionNetwork` class which implements\n",
    "this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LinearRegressionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple linear regression model with only one input variable.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Call the parent constructor (mandatory)\n",
    "        super().__init__()\n",
    "        # Create a \"linear\" attribute which will contain a linear layer with input and\n",
    "        # output of size 1\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Method which implements the model forward pass, ie which takes input data as\n",
    "        argument, applies the model to it and returns the result.\n",
    "        \"\"\"\n",
    "        # Apply our linear layer to input data\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67cbc4-40c1-4629-bec6-503cc51d3e72",
   "metadata": {},
   "source": [
    "Once our class has been created, we can use it to create a model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0147296-3da6-4462-aa1c-9df8ea1d23e9",
   "metadata": {},
   "source": [
    "By displaying a summary description of our model we can see that it has\n",
    "two parameters: the weight and the bias of our single “neuron”. We can\n",
    "see that pytorch take cares of creating these parameters, we don’t have\n",
    "to manually create `w` and `b` tensors anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f5ab37-52ba-4289-8aec-5a042b39b5d6",
   "metadata": {},
   "source": [
    "Once our model class has been created and our model object ha been\n",
    "instanciated, we can build our training process. As seen previously, we\n",
    "will use `MSELoss()` as loss function, and an `SGD` optimizer with a\n",
    "learning rate of 0.1. However, instead of explicitly passing a list of\n",
    "parameters as first optimizer argument, we will use `model.parameters()`\n",
    "which will automatically provide all the parameters of our `model`\n",
    "object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1bb693-0d94-4466-a60c-fb755e030cbf",
   "metadata": {},
   "source": [
    "Finally, we define and run our training loop for a certain number of\n",
    "epochs:\n",
    "\n",
    "-   we start by resetting our gradient with `optimizer.zero_grad()`\n",
    "-   we compute the predicted values by applying our `model` object to\n",
    "    the input data (forward pass)\n",
    "-   we compute the loss value\n",
    "-   we compute the loss gradient for each parameter (backpropagation)\n",
    "-   finally we adjust our model parameters by calling `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode - important for batch normalization and dropout\n",
    "    # layers. Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted values\n",
    "    y_pred = model(x)\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Parameters adjustment\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print results for this epoch. We can get the weight and bias values by accessing the\n",
    "    # \"weight\" and \"bias\" attributes of the model.linear layer\n",
    "    print(\n",
    "        f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model.linear.weight.item():5.2f},\"\n",
    "        f\" bias: {model.linear.bias.item():6.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773eb60-d3ef-407f-be56-d3e51d6af783",
   "metadata": {},
   "source": [
    "We can see that our training process seems to converge towards the\n",
    "“true” values computed above.\n",
    "\n",
    "## Regression with two explanatory variables\n",
    "\n",
    "If we want to do a linear regression with two explanatory variables, our\n",
    "input data `X` will now be a tensor with two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data\n",
    "temperature = [-1.5, 0.2, 3.4, 4.1, 7.8, 13.4, 18.0, 21.5, 32.0, 33.5]\n",
    "humidity = [50.1, 34.8, 51.3, 64.1, 47.8, 53.4, 58.0, 71.5, 32.0, 43.5]\n",
    "X = preprocessing.scale(np.array([temperature, humidity]).transpose())\n",
    "X = torch.tensor(X).float()\n",
    "\n",
    "# Target values\n",
    "icecream = [100.5, 110.2, 133.5, 141.2, 172.8, 225.1, 251.0, 278.9, 366.7, 369.9]\n",
    "y = torch.tensor(icecream).float().view(-1, 1)\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3a7f99-3e45-46c5-a276-0c76296aae6a",
   "metadata": {},
   "source": [
    "As previously, we will create a new class representing our model, with\n",
    "an `__init__()` and a `forward()` methods. The class is almost\n",
    "identical, except that our `Linear` layer will now have 2 inputs instead\n",
    "of 1 (but still 1 output only)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionNetwork2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features=2, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model = LinearRegressionNetwork2()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457b868-3ad8-4020-a3e0-83cf13271963",
   "metadata": {},
   "source": [
    "We can see that our model now has 3 parameters: two weights (one for\n",
    "each input) and one bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815886f-4a69-4fe6-b74f-1ff37624d2d5",
   "metadata": {},
   "source": [
    "The training loop is the same as the previous one. The only difference\n",
    "is that `model.linear.weight` now contains two values instead of one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "learning_rate = 0.1\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  # type: ignore\n",
    "\n",
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    # Set the model to training mode - important for batch normalization and dropout\n",
    "    # layers. Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: compute predicted values\n",
    "    y_pred = model(X)\n",
    "    # Copute loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    # Parameters adjustment\n",
    "    optimizer.step()\n",
    "\n",
    "    print(\n",
    "        f\"{epoch + 1:2}. loss: {loss:7.1f}, weight: {model.linear.weight.data},\"\n",
    "        f\" bias: {model.linear.bias.item():6.1f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1389e-afaa-45f6-b6be-284ce8304c58",
   "metadata": {},
   "source": [
    "### Generalization to any number of explanatory variables\n",
    "\n",
    "**Exercise**\n",
    "\n",
    "We created two different classes above: one for a linear regression\n",
    "model with only one explanatory variable, and one for two explanatory\n",
    "variables. Now we will try to create a more generic model class that can\n",
    "return models accepting any number of explanatory variables.\n",
    "\n",
    "-   Create a new `GeneralLinearRegressionNetwork` class by starting from\n",
    "    the `LinearRegressionNetwork` class seen above\n",
    "-   Modify the `__init__()` method so that it accepts a new argument\n",
    "    called `n_variables`\n",
    "-   Modify the `self.linear` creation so that it takes into account the\n",
    "    value passed as `n_variables` argument\n",
    "\n",
    "Once the class has been created:\n",
    "\n",
    "-   instanciate a model object called `model1` which accepts input data\n",
    "    with one column and apply it to the `x` input data\n",
    "-   instanciate a model object called `model2` which accepts input data\n",
    "    with two columns and apply it to the `X` input data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
