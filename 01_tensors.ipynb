{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe365db-fba7-4c27-b309-e6fd4965e65e",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "``` shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from adl.tensors import plot_points1d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf1037f-7314-457f-8db9-bc97d4042297",
   "metadata": {},
   "source": [
    "Tensors are one of the basic data structures in pytorch. Basically they\n",
    "are numerical arrays that can be processed by different type of devices\n",
    "(CPU, GPU…).\n",
    "\n",
    "A tensor can be created from a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_list = [3.0, 5.0, -4.0]\n",
    "x = torch.tensor(python_list)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e0e18-b75e-42a1-af1b-31a8b0a75961",
   "metadata": {},
   "source": [
    "Computations on tensors are *vectorized*, which means that operations\n",
    "are performed on the entire tensor at once. For example, adding a value\n",
    "to a tensor will add it to each of its elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.0, 5.0, -4.0])\n",
    "print(x + 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(x - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4e65c2-40a0-461b-900b-8b9541954004",
   "metadata": {},
   "source": [
    "Pytorch provides numerous functions to compute on tensors. In general\n",
    "they can be called either as a pytorch function or as a tensor method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.mean())\n",
    "print(torch.mean(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedad0e1-f54b-4170-82d7-27dd1513fd83",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "A temperature in Fahrenheit can be converted in Celsius with the\n",
    "following formula:\n",
    "\n",
    "$$T_{\\textrm{Celsius}} = (T_{\\textrm{Fahrenheit}} - 32) \\times \\frac{5}{9}$$\n",
    "\n",
    "Create a Python function called `fahrenheit_to_celsius` which takes a\n",
    "Fahrenheit temperatures tensor as input and returns its value in\n",
    "Celsius. Apply the function to a tensor with the values\n",
    "`[0, 32, 50, 100]`.\n",
    "\n",
    "## Tensors gradients\n",
    "\n",
    "When creating a tensor, if we specify `requires_grad=True` then every\n",
    "object created by applying a `torch` operation to it will itself be a\n",
    "tensor which keeps track of the functions to apply to compute the\n",
    "gradient of these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = torch.tensor(1.0, requires_grad=True)\n",
    "w1_add = w1 + 4\n",
    "w1_add\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = torch.tensor(2.0, requires_grad=True)\n",
    "step1 = w2 + 4\n",
    "step2 = torch.square(step1)\n",
    "print(step1)\n",
    "print(step2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e7523e-25a1-46f8-920a-578013a7370d",
   "metadata": {},
   "source": [
    "Input tensor gradients can then be computed by applying the `backward()`\n",
    "method to an output tensor. They are stored as the `grad` attribute of\n",
    "the input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "w3 = torch.tensor(1.0, requires_grad=True)\n",
    "w3_mult = 2 * w3 + 1\n",
    "w3_mult.backward()\n",
    "print(w3.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01e36cd-9da3-4418-b6f8-3a94bec7e275",
   "metadata": {},
   "source": [
    "This result can be read as “the gradient value of the function which\n",
    "computes `w3_mult` from `w3`, when `w3` equals 1”. Here the function is\n",
    "`2*w3 + 1`, so its gradient is always 2.\n",
    "\n",
    "If several functions are applied to a tensor, pytorch will keep track of\n",
    "them and compute the gradient value of the original tensor accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "step2.backward()\n",
    "print(w2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760200a1-bcda-4a65-9aeb-70e98c221d41",
   "metadata": {},
   "source": [
    "This result can be seen read as “the gradient value of the function\n",
    "which computes `step2` from `w2`, when `w2` equals 2”. Here the function\n",
    "is `(w2 + 4)²`, so its gradient function is `2*w2 + 8`, and the value of\n",
    "this function when `w2` equals 2 is 12.\n",
    "\n",
    "**Exercise 2**\n",
    "\n",
    "Using tensors:\n",
    "\n",
    "-   compute the gradient of the function $1 /\\log{x}$ when $x$ is 10.\n",
    "-   compute the gradient of the function `fahrenheit_to_celsius` defined\n",
    "    in the previous exercise, when $t$ is 0\n",
    "\n",
    "When the gradient is computed on a function which only takes one\n",
    "parameter, it is the same thing as the value of the derivative of the\n",
    "function for a given parameter value.\n",
    "\n",
    "When the function takes several arguments, each argument `grad` value is\n",
    "the partial derivative of the function at the given argument value. The\n",
    "gradient of the function is the set of partial derivatives for all its\n",
    "arguments.\n",
    "\n",
    "To illustrate this, in the following example:\n",
    "\n",
    "-   `x1.grad` is the partial derivative of $x_1 / x_2$ over $x_1$ when\n",
    "    $x_1 = 3$ and $x_2 = 5$.\n",
    "-   `x2.grad` is the partial derivative of $x_1 / x_2$ over $x_2$ when\n",
    "    $x_1 = 3$ and $x_2 = 5$.\n",
    "-   `(x1.grad, x2.grad)` is the gradient of $x_1 / x_2$ when $x_1 = 3$\n",
    "    and $x_2 = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = torch.tensor(3.0, requires_grad=True)\n",
    "x2 = torch.tensor(5.0, requires_grad=True)\n",
    "res = x1 / x2\n",
    "res.backward()\n",
    "print(x1.grad, x2.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1504c66-a800-45cf-8c87-b7971379edd0",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "-   Create a function named `test_fn` which takes two arguments `x1` and\n",
    "    `x2` and computes $\\sqrt{x_1} / e^{x_2}$.\n",
    "-   Compute the gradient of `test_fn` when $x_1 = 1$ and $x_2 = 2$\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Imagine we have a tensor `x` of 6 numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([-5.0, -2.0, 1.0, 3.0, 5.0, 15.0, 18.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ef45b-7114-4775-bc88-224979412b16",
   "metadata": {},
   "source": [
    "Using a predefined function, we plot these values along an axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points1d(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861e7ab0-a14d-4ae4-87e5-7025155ab186",
   "metadata": {},
   "source": [
    "Now suppose we want to find the value of a parameter `w` for which the\n",
    "sum of the squared distances between the values of `x` and `w` is\n",
    "minimal.\n",
    "\n",
    "For example, we could start with an arbitrary value of `w` at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points1d(x, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae92cda-943d-4ff7-9244-323fd6be9426",
   "metadata": {},
   "source": [
    "To see if our current value for $w=0$ is minimal, we can compute the\n",
    "gradient of the function that computes the sum of squared distances\n",
    "between `x` values and `w` when $w=0$.\n",
    "\n",
    "We already saw how to do that with pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-41",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "y = (x - w).square().sum()\n",
    "y.backward()\n",
    "print(w.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a37ef3-8fd2-4884-b8e8-57e8e7a5bdc5",
   "metadata": {},
   "source": [
    "As we will do this computation several times, we will create a new\n",
    "function `eval_w_squared`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_w_squared(w_value):\n",
    "    x = torch.tensor([-5.0, -2.0, 1.0, 3.0, 5.0, 15.0, 18.0])\n",
    "    w = torch.tensor(w_value, requires_grad=True)\n",
    "    y = (x - w).square().sum()\n",
    "    y.backward()\n",
    "    print(f\"Sum of squared distances: {y}\")\n",
    "    print(f\"Gradient: {w.grad}\")\n",
    "\n",
    "\n",
    "eval_w_squared(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd0324f-1aee-4ef5-a41f-d97382e3d4af",
   "metadata": {},
   "source": [
    "So the sum of squared distances when $w=0$ is 613.0, and its gradient\n",
    "value is -70. This tells us two things:\n",
    "\n",
    "1.  we are not at an optimum value, because the gradient is not equal to\n",
    "    0\n",
    "2.  the gradient value gives us the “direction” in which we have to\n",
    "    adjust our parameter value if we want our function to increase. When\n",
    "    $w=0$ the gradient value is negative, which means that if we\n",
    "    decrease the value of $w$ a bit, the sum of squared distances will\n",
    "    go up. In the contrary, if we increase $w$ a bit, the sum of squared\n",
    "    distances will go down.\n",
    "\n",
    "We want our sum of squared distances to be minimal, so we will try with\n",
    "a greater `w` value, say 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points1d(x, 2)\n",
    "eval_w_squared(2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecd2116-4a9a-4531-a72f-b1958c02df89",
   "metadata": {},
   "source": [
    "When $w=2$, the sum of squared distances is lower, but the gradient\n",
    "value is still negative, so we try again with a greater value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points1d(x, 6)\n",
    "eval_w_squared(6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeab763-d375-48e2-a4de-c6198304e26b",
   "metadata": {},
   "source": [
    "When $w=6$ the sum of squared distances is lower, but the gradient is\n",
    "now positive. This means that if we want our sum of squared distances to\n",
    "go down, we have to decrease the value of `w`. Let’s try with 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_points1d(x, 5)\n",
    "eval_w_squared(5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7438a8bb-82c6-499f-8f70-7787ac665cf6",
   "metadata": {},
   "source": [
    "Now our gradient is equal to 0, this means that we may be at a local\n",
    "optimum.\n",
    "\n",
    "In fact, 5 is the mean of `x`, which is by definition the value which\n",
    "minimizes the sum of squared distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da46280-9627-41e9-8759-43fe661a298b",
   "metadata": {},
   "source": [
    "What we just did here, using gradient values to find a minimum value\n",
    "iteratively, is called a *gradient descent*.\n",
    "\n",
    "**Exercise 4**\n",
    "\n",
    "Create a function `eval_w_abs` which is the same as `eval_w_squared`\n",
    "except that it computes the sum of the absolute values of the\n",
    "differences between `x` elements and `w`.\n",
    "\n",
    "Use this function to do a gradient descent and find the value of `w`\n",
    "that minimizes the sum of the absolute values of differences.\n",
    "\n",
    "What statistical function could have been used to find this value\n",
    "directly?\n",
    "\n",
    "## Minimizing a loss function\n",
    "\n",
    "In machine learning or deep learning, a frequent goal is to predict\n",
    "values from input data by adjusting the parameters of a model.\n",
    "\n",
    "For example, the two following python lists give the average temperature\n",
    "by month at the Lyon-Bron weather station in 1924 and in 2024 (source\n",
    "[infoclimat](https://www.infoclimat.fr/stations-meteo/analyses-mensuelles.php?mois=12&annee=2024))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyon1924 = [3.1, 1.3, 7.7, 11.0, 15.7, 18.0, 20.6, 16.7, 16.2, 11.9, 7.3, 3.4]\n",
    "lyon2024 = [5.3, 8.9, 10.9, 12.5, 15.9, 20.5, 23.3, 24.3, 17.4, 15.8, 8.7, 3.8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4ed5f-6dba-4a2c-9973-3d2504348588",
   "metadata": {},
   "source": [
    "Our objective is to predict a 2024 temperature from a 1924 temperature.\n",
    "For this prediction we will use a very simple model: we will add a fixed\n",
    "value to every 1924 temperature in order to be as close as possible to\n",
    "the ones of 2024.\n",
    "\n",
    "With a more formal notation:\n",
    "\n",
    "-   $x$ is our **input data**, *ie* the monthly 1924 temperatures\n",
    "-   $y$ is the **true values** or **target values** we want to predict,\n",
    "    *ie* the monthly 2024 temperatures\n",
    "-   Our model is $y =x + w$, where $w$ is our unique **model parameter**\n",
    "-   We want $w$ to be the value which minimizes the distance between our\n",
    "    predictions and the true values\n",
    "\n",
    "We will start with a $w$ value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(lyon1924)\n",
    "y = torch.tensor(lyon2024)\n",
    "w = torch.tensor(0.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815b2995-49e5-43ec-84a6-5ae0b26e2de7",
   "metadata": {},
   "source": [
    "We can compute what our predicted values would be after applying our\n",
    "model, *ie* after computing $x + w$. These values are called the\n",
    "*predictions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x + w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5341d9cd-3f08-4554-a073-b239ccf36de4",
   "metadata": {},
   "source": [
    "We want these predictions to be as close as possible to the target\n",
    "values, so we compute a distance between $x + w$ and $y$ by summing the\n",
    "squared values of the distance between their elements.\n",
    "\n",
    "We call this distance the **loss** function, the one we want to\n",
    "minimize.\n",
    "\n",
    "Here is the loss value for $w = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-66",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = x + w\n",
    "loss = torch.sum(torch.square(y_pred - y))\n",
    "print(f\"loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ae07c5-f8cc-483e-965f-7e9b40b3032f",
   "metadata": {},
   "source": [
    "As we have already seen, with pytorch we can apply `backward` to our\n",
    "loss result and `w` will then have a new `grad` attribute: this\n",
    "attribute is the gradient value of our loss function when $w=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-68",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()\n",
    "print(f\"Gradient value: {w.grad.item()}\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0036c8b-6bf4-4875-936e-87f08e1193ce",
   "metadata": {},
   "source": [
    "As we will repeat them several times, we will put these three steps\n",
    "(computing the predictions, the loss value and the gradient) in a\n",
    "function `eval_weight`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_weight(x, y, w_value):\n",
    "    w = torch.tensor(w_value, requires_grad=True)\n",
    "    y_pred = x + w\n",
    "    loss = torch.sum(torch.square(y_pred - y))\n",
    "    loss.backward()\n",
    "    print(f\"loss: {loss}, gradient: {w.grad.item()}\")  # type: ignore\n",
    "\n",
    "\n",
    "eval_weight(x, y, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528bde22-8078-4939-bcb1-1a19a17f8a44",
   "metadata": {},
   "source": [
    "As seen above, the gradient value gives the direction in which $w$ must\n",
    "go for the loss to raise. In this case, the gradient is negative so if\n",
    "we decrease $w$, the loss will increase. As we want to minimize the\n",
    "loss, we want to go **in the opposite direction** of the gradient, and\n",
    "thus we want to increase $w$.\n",
    "\n",
    "Let’s try with $w = 2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-72",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_weight(x, y, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54412e65-c2db-4fd8-9520-21c64e06b94e",
   "metadata": {},
   "source": [
    "The gradient is still negative, so to minimize the loss we will have to\n",
    "increase $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-74",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_weight(x, y, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f06090-0c0f-4bad-99bf-11a7d57c042b",
   "metadata": {},
   "source": [
    "This time the gradient is positive, so to lower the loss we will have to\n",
    "decrease $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-76",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_weight(x, y, 2.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae69111c-065e-4d00-a655-9e2b07d1f44b",
   "metadata": {},
   "source": [
    "If we continue this process, we will get closer and closer to the value\n",
    "of $w$ for which the loss is minimal. In fact we could have computed\n",
    "this value directly by getting the mean of the differences between $x$\n",
    "and $y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-78",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(y - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ecef69-0779-4476-8f74-da0ae9194bc9",
   "metadata": {},
   "source": [
    "## Automating the gradient descent process\n",
    "\n",
    "Until now we did the gradient descent “manually”, by selecting new\n",
    "values based on the sign of the gradient at the current value. We will\n",
    "now see how to automate this process a bit more.\n",
    "\n",
    "As a convention, the prediction phase will be defined in a function\n",
    "called `forward`, which takes our input data as argument (here our `x`\n",
    "tensor) and applies transformative operations (our model) to compute the\n",
    "predicted values.\n",
    "\n",
    "To reuse our previous temperatures example, we define our input data\n",
    "`x`, our true values `y`, our model parameter `w`, and a `forward`\n",
    "method which computes predictions by applying our model, *ie* by\n",
    "computing $x + w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-80",
   "metadata": {},
   "outputs": [],
   "source": [
    "lyon1924 = [3.1, 1.3, 7.7, 11.0, 15.7, 18.0, 20.6, 16.7, 16.2, 11.9, 7.3, 3.4]\n",
    "lyon2024 = [5.3, 8.9, 10.9, 12.5, 15.9, 20.5, 23.3, 24.3, 17.4, 15.8, 8.7, 3.8]\n",
    "\n",
    "x = torch.tensor(lyon1924)\n",
    "y = torch.tensor(lyon2024)\n",
    "\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return x + w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17385a89-6cf3-4bfe-9a0b-85ce1968ba27",
   "metadata": {},
   "source": [
    "Next we define our loss function, *ie* a measure of “distance” between\n",
    "our predicted values and the true values. This loss function can be\n",
    "defined manually (as we did previously), but we can also use predefined\n",
    "loss functions provided by pytorch. For example, our loss could use\n",
    "`torch.nn.MSELoss`, which computes the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-82",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71847a7-9e4c-4d70-bf14-7e3b9656b82f",
   "metadata": {},
   "source": [
    "As we did above, the basis of a training step will be to apply `forward`\n",
    "to `x` to compute predictions given the current `w` value, compute the\n",
    "corresponding loss, and then call `backward` to compute the gradient of\n",
    "the loss function given `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "y_pred = forward(x)\n",
    "# Compute loss value\n",
    "loss = loss_fn(y_pred, y)\n",
    "# Compute loss gradient\n",
    "loss.backward()\n",
    "print(f\"loss: {loss}, gradient for w: {w.grad.item()}\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404e6400-9693-4958-88ed-13104ab4a16c",
   "metadata": {},
   "source": [
    "To complete this step and make it a real “training”, we will have to\n",
    "adjust the value of $w$ in the direction opposite to its gradient. The\n",
    "simplest way to do it is to substract from $w$ its gradient value\n",
    "multiplied by a factor called the step size, or **learning rate**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-86",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.3\n",
    "w.data = w.data - step_size * w.grad  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50ffb4-083f-4ad3-af3b-5ddf1f083196",
   "metadata": {},
   "source": [
    "To run the training process, we have to apply these operations a certain\n",
    "number of times called **epochs**: we can use a simple `for` loop to do\n",
    "this.\n",
    "\n",
    "Note that at the end of each training step we have to “reset” the\n",
    "gradient of `w` by calling `w.grad.zero_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-88",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    y_pred = forward(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    w.data = w.data - step_size * w.grad  # type: ignore\n",
    "    print(\n",
    "        f\"epoch: {epoch}, loss: {loss:.3f}, gradient: {w.grad.item():.3f}, w: {w.data.item():.4f}\"  # type: ignore\n",
    "    )\n",
    "    w.grad.zero_()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b342255-4fca-46f4-931f-15eb870fc024",
   "metadata": {},
   "source": [
    "So here is the complete code of our training process. If we run it for a\n",
    "few epochs we can see that it converges towards the $w$ value that\n",
    "minimizes the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data\n",
    "lyon1924 = [3.1, 1.3, 7.7, 11.0, 15.7, 18.0, 20.6, 16.7, 16.2, 11.9, 7.3, 3.4]\n",
    "lyon2024 = [5.3, 8.9, 10.9, 12.5, 15.9, 20.5, 23.3, 24.3, 17.4, 15.8, 8.7, 3.8]\n",
    "\n",
    "# Input data tensor\n",
    "x = torch.tensor(lyon1924)\n",
    "# True values tensor\n",
    "y = torch.tensor(lyon2024)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Number of training steps\n",
    "epochs = 10\n",
    "# Learning rate\n",
    "step_size = 0.3\n",
    "\n",
    "# Model parameter\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "\n",
    "# Method to apply our model, ie compute predicted values from input data\n",
    "def forward(x):\n",
    "    return x + w\n",
    "\n",
    "\n",
    "# Training process\n",
    "for epoch in range(epochs):\n",
    "    # Compute predictions\n",
    "    y_pred = forward(x)\n",
    "    # Compute loss (distance between predictions and targets)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # Backpropagate to compute parameters gradient\n",
    "    loss.backward()\n",
    "    # Adjust parameter value\n",
    "    w.data = w.data - step_size * w.grad  # type: ignore\n",
    "    print(\n",
    "        f\"epoch: {epoch}, loss: {loss:.3f}, gradient: {w.grad.item():.3f}, w: {w.data.item():.4f}\"  # type: ignore\n",
    "    )\n",
    "    # Reset parameter gradient\n",
    "    w.grad.zero_()  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc8502-50a2-4f8e-ba23-0733c3a95c0c",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "We have two python lists which give the measured diameters and\n",
    "perimeters of a certain number of circles.\n",
    "\n",
    "Use Pytorch to run a training process to find the best value of the\n",
    "parameter `w` which allows to predict the perimeters from the diameters.\n",
    "The model to compute the predicted values will $y = x \\times w$.\n",
    "\n",
    "*Hint*: you can use a step size of 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-92",
   "metadata": {},
   "outputs": [],
   "source": [
    "diameters = [1.4, 2.5, 2.0, 4.8, 4.7, 5.2, 1.3, 2.1, 8.3, 7.4]\n",
    "perimeters = [4.4, 7.9, 6.3, 15.1, 14.8, 16.3, 4.1, 6.6, 26.1, 23.2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
