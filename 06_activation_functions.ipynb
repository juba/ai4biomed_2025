{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd33630d-79a5-4692-aec9-669f3fecd903",
   "metadata": {},
   "source": [
    "# Non linearity and activation functions\n",
    "\n",
    "**Note :** to use this notebook in Google Colab, create a new cell with\n",
    "the following line and run it.\n",
    "\n",
    "``` shell\n",
    "!pip install git+https://gitlab.in2p3.fr/jbarnier/ateliers_deep_learning.git\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from adl import activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf1f5ba-1ee6-4e40-9e72-8d0affcb31dc",
   "metadata": {},
   "source": [
    "In the previous notebooks, we used examples where we wanted to model a\n",
    "linear relationship between several variables. But of course, in most\n",
    "cases the relationship will not be linear.\n",
    "\n",
    "Suppose we want to model a relation between a single vector `x` and a\n",
    "target `y`, but now the relationship is sinusoidal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(np.linspace(-np.pi, np.pi, 100).reshape(-1, 1)).float()\n",
    "y = x.sin()\n",
    "\n",
    "plt.plot(x, y, \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868a42fd-8cfb-4099-95d3-d6c5b03f1e85",
   "metadata": {},
   "source": [
    "We first try with a simple linear layer of size 1, which is equivalent\n",
    "to doing a linear regression between `x` and `y`.\n",
    "\n",
    "We first define a model class with a single `nn.Linear` layer with\n",
    "`in_features` and `out_features` of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleLinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daddf979-c484-4e6c-81f1-0c3a30059446",
   "metadata": {},
   "source": [
    "And we instantiate a model object from this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = SingleLinearModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fb6e03-7feb-4a40-93f0-3d3ed9efafc6",
   "metadata": {},
   "source": [
    "We then use predefined functions to train the model and plot the target\n",
    "values and the trained model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = activations.train(linear_model, x, y)\n",
    "activations.plot(x, y, trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd93b77-09fb-4b03-8e03-d89cc184bbdf",
   "metadata": {},
   "source": [
    "We can see that the result is a straight line which is not a good\n",
    "representation of our data.\n",
    "\n",
    "Maybe we could try to improve the model by adding another linear layer\n",
    "with an hidden dimension of size 5? This way it could be able to capture\n",
    "more nuanced relationships?\n",
    "\n",
    "To do this we will modify the `model` attribute of our model class and\n",
    "use `nn.Sequential`, which allows to define a series of layers which\n",
    "will be applied sequentially to our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.Linear(5, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "linear_model = LinearModel()\n",
    "\n",
    "\n",
    "trained_model = activations.train(linear_model, x, y)\n",
    "activations.plot(x, y, trained_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3f135-901b-4089-a922-aac4b99276a0",
   "metadata": {},
   "source": [
    "We can see that the result is exactly the same: that’s because a\n",
    "combination of linear transformations, at the end, is still a linear\n",
    "transformation.\n",
    "\n",
    "## Activation functions\n",
    "\n",
    "To be able to capture non-linear relationships, deep neural networks use\n",
    "*activation functions*, *ie* functions that will introduce non-linearity\n",
    "between layers.\n",
    "\n",
    "There are many available functions, below is a plot of three of them:\n",
    "ReLU, Sigmoid and Tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fns = {\"ReLU\": nn.ReLU(), \"Sigmoid\": nn.Sigmoid(), \"Tanh\": nn.Tanh()}\n",
    "activations.plot_activation_fns(activation_fns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed95b75-6ed5-4b3b-a424-6d8d7f78a7cd",
   "metadata": {},
   "source": [
    "-   The ReLU function will keep all positive values as is, and transform\n",
    "    all negative values to 0.\n",
    "-   The Sigmoid function will map values between 0 and 1\n",
    "-   The Tanh function will map values between -1 and 1\n",
    "\n",
    "In general, activation functions are just functions without parameters\n",
    "that transform their inputs: they don’t “learn” anything during the\n",
    "training process and don’t add any parameter to the model (there are\n",
    "some exceptions like `PReLU`).\n",
    "\n",
    "If we want to add an activation layer to our model, we just have to\n",
    "insert it where we want, for example between our two linear layers.\n",
    "\n",
    "Here is the result if we insert an `nn.ReLU()` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReluModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(5, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "torch.manual_seed(133)\n",
    "\n",
    "relu_model = ReluModel()\n",
    "\n",
    "trained_model = activations.train(relu_model, x, y)\n",
    "activations.plot(x, y, trained_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab74eab-72b6-4592-9944-28d3a88e92c5",
   "metadata": {},
   "source": [
    "We can see that `ReLU` allows to break the linearity by creating some\n",
    "sort of “steps” or “segments” that allow to much better fit our data.\n",
    "\n",
    "We can replace `nn.ReLU` with `nn.Sigmoid()` to use a Sigmoid activation\n",
    "layer instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SigmoidModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1, 5),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(5, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "torch.manual_seed(133)\n",
    "\n",
    "sigmoid_model = SigmoidModel()\n",
    "\n",
    "trained_model = activations.train(sigmoid_model, x, y)\n",
    "activations.plot(x, y, trained_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa0ec4-4538-466f-b9fc-7c28c083c518",
   "metadata": {},
   "source": [
    "Due to the sinusoidal nature of our dataset, the Sigmoid function allows\n",
    "to quite smoothly fit our data relationship.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "We want to use a neural network to model the non-linear relationship\n",
    "between the `xc` and `yc` variables plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "xc = torch.tensor(np.linspace(-2.0, 2.0, 100)).reshape(-1, 1).float()\n",
    "yc = torch.tensor([0.0] * 30 + [1.0] * 30 + [1.5] * 40).reshape(-1, 1)\n",
    "\n",
    "plt.plot(xc, yc, \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5e6b0e-d53c-498a-b9c5-ed6a383a7373",
   "metadata": {},
   "source": [
    "Create the following three models, and for each model train it with the\n",
    "`activations.train()` function and plot the results with the\n",
    "`activations.plot()` function:\n",
    "\n",
    "-   A linear model with two linear layers and an hidden dimension of 10\n",
    "-   A model with two linear layers, an hidden dimension of 10, and a\n",
    "    ReLU activation in between\n",
    "-   A model with two linear layers, an hidden dimension of 3, and a Tanh\n",
    "    activation in between"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
